{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bda9bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_pyg_obj\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import fix_target_shapes,remove_unused_onehot_columns,set_seed,filter_metals\n",
    "from mofstructure import mofdeconstructor\n",
    "\n",
    "from fairmofsyncondition.read_write.coords_library import pytorch_geometric_to_ase\n",
    "\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "\n",
    "convert_struct = {'cubic':0, 'hexagonal':1, 'monoclinic':2, 'orthorhombic':3, 'tetragonal':4,'triclinic':5, 'trigonal':6}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212130",
   "metadata": {},
   "source": [
    "# GNN for Metal Salt Prediction\n",
    "\n",
    "This code implements a **Graph Neural Network (GNN)** to predict metal salts, using:\n",
    "- **Node Features**  \n",
    "- **Edge Features**  \n",
    "- **Lattice**   \n",
    "- **Oms**\n",
    "- **Atomic number**\n",
    "---\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "1. **Load the Data**  \n",
    "   Import and prepare the dataset for use in the model.\n",
    "\n",
    "2. **Define the GNN Model**  \n",
    "   Define the neural network architecture (layers, activation functions, etc.).\n",
    "\n",
    "3. **Train the Model**  \n",
    "   - Train the model on the dataset.  \n",
    "   - Save the trained GNN weights into the `tmp/` folder.  \n",
    "\n",
    "   ⚠️ **Note**:  \n",
    "   If you only want to **test the model** without re-training, you can **skip this section** and avoid running the training step.\n",
    "\n",
    "4. **Load and Evaluate the Model**  \n",
    "   - Load the trained model weights.  \n",
    "   - Evaluate the model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afb0dd",
   "metadata": {},
   "source": [
    "### 1) Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d29a2",
   "metadata": {},
   "source": [
    "\n",
    "### to get oms\n",
    "### stru.get_oms()[\"has_oms\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b02499a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mof_syncondition_data']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"../../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362efd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a8b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17366789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed dataset...\n"
     ]
    }
   ],
   "source": [
    "from mofstructure.structure import MOFstructure\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "convert_metals = {j:i for i,j in enumerate(mofdeconstructor.transition_metals()[1:])}\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(seed=42)\n",
    "\n",
    "data_in = load_pyg_obj(path_to_mdb=\"../../data/mof_syncondition_data/\")\n",
    "dataset = fix_target_shapes(data_in, \"metal_salts\")\n",
    "dataset = remove_unused_onehot_columns(dataset, \"metal_salts\")\n",
    "\n",
    "\n",
    "bad = []\n",
    "good = []\n",
    "\n",
    "if os.path.exists(\"../../dataset_cleen_all_info.pt\"):\n",
    "    print(\"Loading precomputed dataset...\")\n",
    "    dataset = torch.load(\"../../dataset_cleen_all_info.pt\")\n",
    "else:\n",
    "    print(\"Computing dataset with all info...\")\n",
    "    for d in tqdm(dataset):\n",
    "        try:\n",
    "            # =======================\n",
    "            # Parte 1: atomic one-hot\n",
    "            # =======================\n",
    "            node_features = d.x.numpy()\n",
    "            atom_num = node_features[:, 0].astype(int)\n",
    "            a, b = np.unique(atom_num, return_counts=True)\n",
    "            emb = torch.zeros(120)\n",
    "            for aa, bb in zip(a, b):\n",
    "                emb[aa] = bb\n",
    "            d.atomic_one_hot = emb\n",
    "            \n",
    "            # =======================\n",
    "            # Parte 2: struttura ASE\n",
    "            # =======================\n",
    "            ase_atoms = pytorch_geometric_to_ase(d)\n",
    "            stru = MOFstructure(ase_atoms)\n",
    "            pymat = AseAtomsAdaptor.get_structure(ase_atoms)\n",
    "            \n",
    "            # =======================\n",
    "            # Parte 3: OMS\n",
    "            # =======================\n",
    "            emb = torch.zeros(96)\n",
    "            tmp_dict = dict()\n",
    "            for i in stru.get_oms()[\"metal_info\"]:\n",
    "                cord = i[\"coordination_number\"]\n",
    "                metal = i[\"metal\"]\n",
    "\n",
    "                if metal in tmp_dict:\n",
    "                    if cord > tmp_dict[metal]:\n",
    "                        tmp_dict[metal] = cord\n",
    "                else:\n",
    "                    tmp_dict[metal] = cord\n",
    "\n",
    "            for i, j in tmp_dict.items():\n",
    "                emb[convert_metals[i]] = j\n",
    "            d.cordinates = emb\n",
    "            \n",
    "            # =======================\n",
    "            # Parte 4: spazio e sistema cristallino\n",
    "            # =======================\n",
    "            sga = SpacegroupAnalyzer(pymat)\n",
    "            space_group_number = sga.get_space_group_number()\n",
    "            emb = torch.zeros(231)\n",
    "            emb[space_group_number] = 1\n",
    "            d.space_group_number = emb\n",
    "\n",
    "            get_crystal_system = sga.get_crystal_system()\n",
    "            emb = torch.zeros(7)\n",
    "            emb[convert_struct[get_crystal_system]] = 1\n",
    "            d.crystal_system = emb\n",
    "            # =======================\n",
    "            # Parte 5: altri attributi\n",
    "            # =======================\n",
    "            d.oms = d.oms.view(1, 1).float()\n",
    "\n",
    "            ###################### no porosity is too long to compute\n",
    "            #por = stru.get_porosity()\n",
    "            #por = list(por.values())\n",
    "            #d.porosity = torch.tensor(por)\n",
    "\n",
    "            d.modified_scherrer = None\n",
    "            d.microstrain = None\n",
    "\n",
    "            # Se arrivo qui senza eccezioni → struttura buona\n",
    "            good.append(d)\n",
    "        except Exception:\n",
    "            bad.append(d)\n",
    "            continue\n",
    "    torch.save(good, \"../../dataset_cleen_all_info.pt\")   # salva lista di Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1ec87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are classes 122 3054\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ==================== Utils & setup ====================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Filtra classi rare (come nel tuo codice)\n",
    "Y = [d.metal_salts.argmax(dim=1).item() for d in dataset]\n",
    "a,b = np.unique(Y, return_counts=True)\n",
    "conv_y = {i:j for i,j in zip(a,b)}\n",
    "good = [d for d in dataset if conv_y[d.metal_salts.argmax(dim=1).item()] > 5]\n",
    "dataset = good\n",
    "\n",
    "print(\"There are classes\", len(np.unique([d.metal_salts.argmax(dim=1).item() for d in dataset])), len(dataset))\n",
    "\n",
    "# Split & loaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=128)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=128)\n",
    "\n",
    "# ==================== NEW: ablation helpers ====================\n",
    "\n",
    "# 1) Getter per ciascuna feature extra (sempre reshaped a [B, D])\n",
    "#    Aggiungi qui dentro eventuali nuove feature future.\n",
    "\n",
    "def _reshape_feat(tensor, d):\n",
    "    # Se è Batch (ha num_graphs)\n",
    "    if hasattr(d, \"num_graphs\"):\n",
    "        return tensor.view(d.num_graphs, -1)\n",
    "    else:  # è un singolo Data\n",
    "        return tensor.view(1, -1)\n",
    "\n",
    "EXTRA_GETTERS = {\n",
    "    \"atomic_one_hot\":      lambda d: _reshape_feat(d.atomic_one_hot, d),\n",
    "    \"space_group_number\":  lambda d: _reshape_feat(d.space_group_number, d),\n",
    "    \"crystal_system\":      lambda d: _reshape_feat(d.crystal_system, d),\n",
    "    \"oms\":                 lambda d: _reshape_feat(d.oms, d),\n",
    "    \"cordinates\":          lambda d: _reshape_feat(d.cordinates, d),\n",
    "}\n",
    "def compute_extras_dim(sample_data, selected_extras):\n",
    "    dim = 0\n",
    "    for name in selected_extras:\n",
    "        if name not in EXTRA_GETTERS:\n",
    "            raise ValueError(f\"Feature extra sconosciuta: {name}\")\n",
    "        dim += EXTRA_GETTERS[name](sample_data).shape[1]\n",
    "    return dim\n",
    "\n",
    "def build_extras_tensor(data, selected_extras):\n",
    "    if not selected_extras:\n",
    "        return None\n",
    "    parts = [EXTRA_GETTERS[name](data) for name in selected_extras]\n",
    "    return torch.cat(parts, dim=1)\n",
    "\n",
    "def extras_suffix(selected_extras):\n",
    "    if not selected_extras:\n",
    "        return \"no_extras\"\n",
    "    return \"_\".join(selected_extras)\n",
    "\n",
    "# ==================== Model ====================\n",
    "\n",
    "class MetalSaltGNN_Ablation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim,\n",
    "        edge_in_dim,\n",
    "        lattice_in_dim=9,\n",
    "        hidden_dim=128,\n",
    "        num_classes=10,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=None,      # NEW: lista di nomi feature extra\n",
    "        extras_dim=0               # NEW: dimensione totale delle extra\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout = dropout\n",
    "        self.selected_extras = selected_extras or []\n",
    "        self.extras_dim = extras_dim\n",
    "\n",
    "        # --- Edge encoder (per GINE)\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # --- GINE layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_bns = nn.ModuleList() if use_batchnorm else None\n",
    "        for i in range(num_gnn_layers):\n",
    "            in_dim = node_in_dim if i == 0 else hidden_dim\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.gnn_layers.append(GINEConv(mlp, edge_dim=hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.gnn_bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # --- Lattice encoder\n",
    "        lattice_layers = []\n",
    "        in_dim = lattice_in_dim\n",
    "        for _ in range(max(1, num_lattice_layers - 1)):\n",
    "            lattice_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                lattice_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "        lattice_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        self.lattice_encoder = nn.Sequential(*lattice_layers)\n",
    "\n",
    "        # --- Final MLP head\n",
    "        final_in = hidden_dim * 2 + self.extras_dim  # graph pooled + lattice + extras\n",
    "        mlp_layers = []\n",
    "        in_dim = final_in\n",
    "        for _ in range(max(1, num_mlp_layers - 1)):\n",
    "            mlp_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                mlp_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            mlp_layers.append(nn.Dropout(p=dropout))\n",
    "            in_dim = hidden_dim\n",
    "        mlp_layers.append(nn.Linear(in_dim, num_classes))\n",
    "        self.final_mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode edges\n",
    "        e = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # GNN layers\n",
    "        for i, conv in enumerate(self.gnn_layers):\n",
    "            x = conv(x, edge_index, e)\n",
    "            x = F.relu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.gnn_bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        x_pool = global_mean_pool(x, batch)\n",
    "\n",
    "        # Lattice encoding (sempre usato)\n",
    "        lattice = data.lattice.view(-1, 9)\n",
    "        lattice_feat = self.lattice_encoder(lattice)\n",
    "\n",
    "        # Extras (abilitate in base alla lista)\n",
    "        extras = build_extras_tensor(data, self.selected_extras)\n",
    "        if extras is not None:\n",
    "            final_in = torch.cat([x_pool, lattice_feat, extras], dim=1)\n",
    "        else:\n",
    "            final_in = torch.cat([x_pool, lattice_feat], dim=1)\n",
    "\n",
    "        out = self.final_mlp(final_in)\n",
    "        return out\n",
    "\n",
    "# ==================== Train / Eval helpers ====================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, target_name):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = torch.argmax(data[target_name], dim=1).long()\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, target_name):\n",
    "    model.eval()\n",
    "    correct = {1: 0, 3: 0, 5: 0, 10: 0}\n",
    "    total = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data)\n",
    "        labels = torch.argmax(data[target_name], dim=1).long()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        _, pred = logits.topk(10, dim=1)\n",
    "        for k in correct.keys():\n",
    "            correct[k] += (pred[:, :k] == labels.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "        top1_preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(top1_preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    macro_f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average=\"macro\")\n",
    "\n",
    "    results = {f\"top{k}_acc\": correct[k] / max(1, total) for k in correct}\n",
    "    results[\"macro_f1\"] = macro_f1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b26659",
   "metadata": {},
   "source": [
    "### 2) Define the GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b59c9e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atomic_one_hot', 'cordinates', 'crystal_system', 'oms', 'space_group_number']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==================== Run ablation ====================\n",
    "\n",
    "node_in_dim = dataset[0].x.shape[1]\n",
    "edge_in_dim = dataset[0].edge_attr.shape[1]\n",
    "lattice_in_dim = 9\n",
    "\n",
    "# Scegli qui le extra da usare nell’ablation:\n",
    "# Esempio richiesto: [\"oms\", \"atomic_one_hot\"]\n",
    "\n",
    "\n",
    "EXTRA_GETTERS = {\n",
    "    \"atomic_one_hot\":      lambda d: _reshape_feat(d.atomic_one_hot, d),\n",
    "    \"space_group_number\":  lambda d: _reshape_feat(d.space_group_number, d),\n",
    "    \"crystal_system\":      lambda d: _reshape_feat(d.crystal_system, d),\n",
    "    \"oms\":                 lambda d: _reshape_feat(d.oms, d),\n",
    "    \"cordinates\":          lambda d: _reshape_feat(d.cordinates, d),\n",
    "}\n",
    "\n",
    "selected_extras = []\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\"]\n",
    "selected_extras = [\"cordinates\"]\n",
    "selected_extras = [\"crystal_system\"]\n",
    "selected_extras = [\"oms\"]\n",
    "selected_extras = [\"space_group_number\"]\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\",\"cordinates\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"crystal_system\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"space_group_number\"]\n",
    "\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"cordinates\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"crystal_system\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"space_group_number\"]\n",
    "\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"cordinates\", \"crystal_system\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"cordinates\", \"space_group_number\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"cordinates\",\"crystal_system\", \"oms\",\"space_group_number\"]\n",
    "# imoprtant: for saving\n",
    "selected_extras = np.sort(selected_extras).tolist()\n",
    "selected_extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "932254da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo dinamico della dimensione delle extra\n",
    "extras_dim = compute_extras_dim(dataset[0], selected_extras)\n",
    "\n",
    "# Classi\n",
    "Y_size = max([torch.argmax(d[\"metal_salts\"]).item() for d in dataset])\n",
    "num_classes = Y_size + 1\n",
    "\n",
    "number_of_runs = [1,2,3,4,5]\n",
    "hidden_dim = 64\n",
    "dropout = 0.35\n",
    "\n",
    "results = []\n",
    "suffix = extras_suffix(selected_extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24df511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666, 455)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes,extras_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be2841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training config: HID64_DO0.35_SEED1__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4525, top5_acc=0.6984, top3_acc=0.6328 macro_f1=0.1590\n",
      "VAL: top1_acc=0.4984, top5_acc=0.7738, top3_acc=0.6918 macro_f1=0.1921\n",
      "VAL: top1_acc=0.4984, top5_acc=0.7902, top3_acc=0.7016 macro_f1=0.1935\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8164, top3_acc=0.7082 macro_f1=0.2104\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8066, top3_acc=0.6984 macro_f1=0.2369\n",
      "VAL: top1_acc=0.5180, top5_acc=0.7967, top3_acc=0.6984 macro_f1=0.2564\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2476\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8098, top3_acc=0.7246 macro_f1=0.2424\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8131, top3_acc=0.7213 macro_f1=0.2518\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8230, top3_acc=0.7246 macro_f1=0.2398\n",
      "VAL: top1_acc=0.5377, top5_acc=0.8262, top3_acc=0.7311 macro_f1=0.2573\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8066, top3_acc=0.7082 macro_f1=0.2572\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8197, top3_acc=0.7180 macro_f1=0.2534\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8426, top3_acc=0.7344 macro_f1=0.2668\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8230, top3_acc=0.7410 macro_f1=0.2337\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8131, top3_acc=0.7213 macro_f1=0.2839\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8131, top3_acc=0.7180 macro_f1=0.2903\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8197, top3_acc=0.7344 macro_f1=0.2847\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8033, top3_acc=0.7148 macro_f1=0.2702\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8033, top3_acc=0.7311 macro_f1=0.2642\n",
      "VAL: top1_acc=0.5344, top5_acc=0.8066, top3_acc=0.7344 macro_f1=0.2942\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8164, top3_acc=0.7148 macro_f1=0.2994\n",
      "VAL: top1_acc=0.5377, top5_acc=0.8164, top3_acc=0.7311 macro_f1=0.3145\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8131, top3_acc=0.7115 macro_f1=0.2946\n",
      "Early stopping at epoch 120\n",
      "HID64_DO0.35_SEED1__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4739, top5_acc=0.7745, top3_acc=0.6601 macro_f1=0.2989\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED2__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4295, top5_acc=0.7213, top3_acc=0.6197 macro_f1=0.0986\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7607, top3_acc=0.6557 macro_f1=0.1765\n",
      "VAL: top1_acc=0.4918, top5_acc=0.7639, top3_acc=0.6754 macro_f1=0.1857\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7869, top3_acc=0.6984 macro_f1=0.2348\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8000, top3_acc=0.6918 macro_f1=0.2202\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8131, top3_acc=0.6984 macro_f1=0.2508\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2114\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2187\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8328, top3_acc=0.7115 macro_f1=0.2362\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8262, top3_acc=0.7344 macro_f1=0.2209\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8328, top3_acc=0.7279 macro_f1=0.2454\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8295, top3_acc=0.7246 macro_f1=0.2555\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8393, top3_acc=0.7279 macro_f1=0.2353\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8295, top3_acc=0.7246 macro_f1=0.2229\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8328, top3_acc=0.7148 macro_f1=0.2153\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8393, top3_acc=0.7475 macro_f1=0.2554\n",
      "VAL: top1_acc=0.5344, top5_acc=0.8361, top3_acc=0.7475 macro_f1=0.2561\n",
      "VAL: top1_acc=0.5410, top5_acc=0.8328, top3_acc=0.7377 macro_f1=0.2636\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8361, top3_acc=0.7148 macro_f1=0.2402\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8131, top3_acc=0.7279 macro_f1=0.2462\n",
      "VAL: top1_acc=0.5410, top5_acc=0.8393, top3_acc=0.7377 macro_f1=0.2841\n",
      "VAL: top1_acc=0.5246, top5_acc=0.8295, top3_acc=0.7410 macro_f1=0.2848\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8361, top3_acc=0.7279 macro_f1=0.2580\n",
      "Early stopping at epoch 115\n",
      "HID64_DO0.35_SEED2__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4641, top5_acc=0.7908, top3_acc=0.6961 macro_f1=0.2821\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED3__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4459, top5_acc=0.7148, top3_acc=0.6426 macro_f1=0.1219\n",
      "VAL: top1_acc=0.5016, top5_acc=0.7672, top3_acc=0.6820 macro_f1=0.1890\n",
      "VAL: top1_acc=0.5115, top5_acc=0.7934, top3_acc=0.6984 macro_f1=0.2115\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8164, top3_acc=0.7180 macro_f1=0.2320\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8098, top3_acc=0.7213 macro_f1=0.2274\n",
      "VAL: top1_acc=0.5311, top5_acc=0.7934, top3_acc=0.7246 macro_f1=0.2316\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8164, top3_acc=0.7246 macro_f1=0.2488\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8131, top3_acc=0.7016 macro_f1=0.2249\n",
      "VAL: top1_acc=0.5377, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2524\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8033, top3_acc=0.6984 macro_f1=0.2512\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8066, top3_acc=0.7180 macro_f1=0.2654\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8197, top3_acc=0.7115 macro_f1=0.2543\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2833\n",
      "VAL: top1_acc=0.5311, top5_acc=0.7836, top3_acc=0.7016 macro_f1=0.2560\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8098, top3_acc=0.7082 macro_f1=0.2492\n",
      "VAL: top1_acc=0.5344, top5_acc=0.8361, top3_acc=0.7377 macro_f1=0.2892\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8131, top3_acc=0.7410 macro_f1=0.2648\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8131, top3_acc=0.6885 macro_f1=0.2219\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8131, top3_acc=0.7213 macro_f1=0.2321\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8098, top3_acc=0.7213 macro_f1=0.2426\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8066, top3_acc=0.7082 macro_f1=0.2603\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7967, top3_acc=0.6918 macro_f1=0.2564\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8164, top3_acc=0.7279 macro_f1=0.2584\n",
      "VAL: top1_acc=0.4820, top5_acc=0.8197, top3_acc=0.7213 macro_f1=0.2277\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2579\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8066, top3_acc=0.7049 macro_f1=0.2349\n",
      "Early stopping at epoch 130\n",
      "HID64_DO0.35_SEED3__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4771, top5_acc=0.7680, top3_acc=0.7092 macro_f1=0.2970\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED4__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4689, top5_acc=0.6918, top3_acc=0.6164 macro_f1=0.1345\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7738, top3_acc=0.6689 macro_f1=0.1767\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7803, top3_acc=0.6885 macro_f1=0.1869\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7967, top3_acc=0.6951 macro_f1=0.1940\n",
      "VAL: top1_acc=0.5115, top5_acc=0.7967, top3_acc=0.6918 macro_f1=0.2045\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8033, top3_acc=0.7115 macro_f1=0.2212\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7902, top3_acc=0.7016 macro_f1=0.2126\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8098, top3_acc=0.6885 macro_f1=0.2255\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8197, top3_acc=0.6951 macro_f1=0.2226\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8230, top3_acc=0.7443 macro_f1=0.2421\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8262, top3_acc=0.7180 macro_f1=0.2329\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8098, top3_acc=0.7148 macro_f1=0.2187\n",
      "VAL: top1_acc=0.5082, top5_acc=0.7967, top3_acc=0.6852 macro_f1=0.2360\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8328, top3_acc=0.7016 macro_f1=0.2465\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8131, top3_acc=0.7180 macro_f1=0.2293\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8426, top3_acc=0.7377 macro_f1=0.2781\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8098, top3_acc=0.7148 macro_f1=0.2139\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8361, top3_acc=0.7541 macro_f1=0.2609\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8230, top3_acc=0.7279 macro_f1=0.2553\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8230, top3_acc=0.7443 macro_f1=0.2492\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8361, top3_acc=0.7508 macro_f1=0.2549\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8098, top3_acc=0.7148 macro_f1=0.2550\n",
      "VAL: top1_acc=0.4951, top5_acc=0.8164, top3_acc=0.7311 macro_f1=0.2576\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8426, top3_acc=0.7377 macro_f1=0.2581\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8295, top3_acc=0.7279 macro_f1=0.2898\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8328, top3_acc=0.7246 macro_f1=0.2644\n",
      "Early stopping at epoch 130\n",
      "HID64_DO0.35_SEED4__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4608, top5_acc=0.7745, top3_acc=0.6699 macro_f1=0.2875\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED5__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4393, top5_acc=0.7016, top3_acc=0.6295 macro_f1=0.1284\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7574, top3_acc=0.6557 macro_f1=0.1799\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7639, top3_acc=0.6787 macro_f1=0.1963\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7934, top3_acc=0.6984 macro_f1=0.2194\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7869, top3_acc=0.7148 macro_f1=0.2531\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7967, top3_acc=0.6820 macro_f1=0.2410\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8033, top3_acc=0.7049 macro_f1=0.2308\n",
      "VAL: top1_acc=0.5410, top5_acc=0.8066, top3_acc=0.7115 macro_f1=0.2863\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8197, top3_acc=0.7180 macro_f1=0.2253\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8098, top3_acc=0.7049 macro_f1=0.2316\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8164, top3_acc=0.7148 macro_f1=0.2605\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8066, top3_acc=0.7213 macro_f1=0.2600\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8262, top3_acc=0.7246 macro_f1=0.2372\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8328, top3_acc=0.7213 macro_f1=0.2555\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8098, top3_acc=0.7344 macro_f1=0.2458\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8328, top3_acc=0.7344 macro_f1=0.2776\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8131, top3_acc=0.7049 macro_f1=0.2598\n",
      "VAL: top1_acc=0.4721, top5_acc=0.8066, top3_acc=0.7344 macro_f1=0.2528\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8361, top3_acc=0.7180 macro_f1=0.2696\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2562\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8197, top3_acc=0.6984 macro_f1=0.2598\n",
      "VAL: top1_acc=0.4820, top5_acc=0.8230, top3_acc=0.7180 macro_f1=0.2650\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8066, top3_acc=0.7344 macro_f1=0.2798\n",
      "VAL: top1_acc=0.4951, top5_acc=0.8131, top3_acc=0.7180 macro_f1=0.2379\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8361, top3_acc=0.7148 macro_f1=0.2723\n",
      "VAL: top1_acc=0.4885, top5_acc=0.8164, top3_acc=0.7049 macro_f1=0.2713\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8295, top3_acc=0.7082 macro_f1=0.2700\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8393, top3_acc=0.7311 macro_f1=0.2505\n",
      "VAL: top1_acc=0.4885, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2465\n",
      "VAL: top1_acc=0.4361, top5_acc=0.8230, top3_acc=0.7148 macro_f1=0.2507\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8197, top3_acc=0.7148 macro_f1=0.2813\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8164, top3_acc=0.7344 macro_f1=0.2892\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8131, top3_acc=0.7115 macro_f1=0.2577\n",
      "VAL: top1_acc=0.5016, top5_acc=0.7967, top3_acc=0.7049 macro_f1=0.2437\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8328, top3_acc=0.7279 macro_f1=0.2552\n",
      "VAL: top1_acc=0.4787, top5_acc=0.8000, top3_acc=0.6820 macro_f1=0.2626\n",
      "VAL: top1_acc=0.4885, top5_acc=0.8131, top3_acc=0.6885 macro_f1=0.2805\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7869, top3_acc=0.6951 macro_f1=0.2974\n",
      "Early stopping at epoch 190\n",
      "HID64_DO0.35_SEED5__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4510, top5_acc=0.7908, top3_acc=0.6895 macro_f1=0.2857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__{suffix}\"\n",
    "    print(f\"\\n===== Training config: {config_name} =====\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = MetalSaltGNN_Ablation(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=selected_extras,\n",
    "        extras_dim=extras_dim\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    checkpoint_name = f\"trained_models/Metal_salts_{config_name}.pt\"\n",
    "\n",
    "    best_metric = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50\n",
    "    eval_every = 5\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        _ = train_one_epoch(model, train_loader, criterion, optimizer, device, \"metal_salts\")\n",
    "        if epoch % eval_every == 0:\n",
    "            res = evaluate(model, val_loader, device, \"metal_salts\")\n",
    "            print(f\"VAL: top1_acc={res['top1_acc']:.4f}, top5_acc={res['top5_acc']:.4f}, top3_acc={res['top3_acc']:.4f} macro_f1={res['macro_f1']:.4f}\")\n",
    "\n",
    "            # Early stopping su top5, come nel tuo codice\n",
    "            if res[\"top5_acc\"] > best_metric:\n",
    "                best_metric = res[\"top5_acc\"]\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), checkpoint_name)\n",
    "            else:\n",
    "                epochs_no_improve += eval_every\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # Valutazione test con il best checkpoint\n",
    "    model.load_state_dict(torch.load(checkpoint_name, map_location=device))\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "    results.append({**res_test, 'config': config_name})\n",
    "    print(f\"{config_name} TEST: top1_acc={res_test['top1_acc']:.4f}, top5_acc={res_test['top5_acc']:.4f}, top3_acc={res_test['top3_acc']:.4f} macro_f1={res_test['macro_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617444",
   "metadata": {},
   "source": [
    "# evalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ee7807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Media ± Std sulle run ====\n",
      "\n",
      "top1_acc: 0.4654 ± 0.0094\n",
      "top3_acc: 0.6850 ± 0.0177\n",
      "top5_acc: 0.7797 ± 0.0094\n",
      "top10_acc: 0.8993 ± 0.0084\n",
      "macro_f1: 0.2902 ± 0.0066\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__{suffix}\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    model = MetalSaltGNN_Ablation(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=selected_extras,\n",
    "        extras_dim=extras_dim\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint_name = f\"trained_models/Metal_salts_{config_name}.pt\"\n",
    "    checkpoint = torch.load(checkpoint_name, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "\n",
    "    results.append({\n",
    "        'config': config_name,\n",
    "        'top1_acc':  res_test['top1_acc'],\n",
    "        'top3_acc':  res_test['top3_acc'],\n",
    "        'top5_acc':  res_test['top5_acc'],\n",
    "        'top10_acc': res_test['top10_acc'],\n",
    "        'macro_f1':  res_test['macro_f1'],\n",
    "    })\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "# results è la tua lista di dict\n",
    "metrics = [\"top1_acc\",\"top3_acc\",\"top5_acc\",\"top10_acc\",\"macro_f1\"]\n",
    "\n",
    "print(\"==== Media ± Std sulle run ====\\n\")\n",
    "for metric in metrics:\n",
    "    values = [r[metric] for r in results]\n",
    "    mean = np.mean(values)\n",
    "    std  = np.std(values)\n",
    "    print(f\"{metric}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d770a8",
   "metadata": {},
   "source": [
    "# real word experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19c260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96ff61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b92e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mofstructure import structure, mofdeconstructor\n",
    "from mofstructure.filetyper import load_iupac_names\n",
    "from fairmofsyncondition.read_write import cheminfo2iupac, coords_library, filetyper\n",
    "from ase.data import atomic_numbers\n",
    "from ase.io import read\n",
    "import torch\n",
    "\n",
    "\n",
    "inchi_corrector = {\n",
    "    \"FDTQOZYHNDMJCM-UHFFFAOYSA-N\":\"benzene-1,4-dicarboxylic acid\"\n",
    "}\n",
    "def get_ligand_iupacname(ligand_inchi):\n",
    "    print(ligand_inchi)\n",
    "    name = load_iupac_names().get(ligand_inchi, None)\n",
    "    if name is None:\n",
    "        pubchem = cheminfo2iupac.pubchem_to_inchikey(ligand_inchi, name='inchikey')\n",
    "        if pubchem is None:\n",
    "            name = inchi_corrector.get(ligand_inchi, ligand_inchi)\n",
    "        else:\n",
    "            pubchem.get('iupac_name', ligand_inchi)\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_system(filename):\n",
    "    \"\"\"\n",
    "    A function to extract\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    os.makedirs('LigandsXYZ', exist_ok=True)\n",
    "    ase_data = read(filename)\n",
    "    structure_data = structure.MOFstructure(ase_atoms=ase_data)\n",
    "    _, ligands = structure_data.get_ligands()\n",
    "\n",
    "    inchikeys = [ligand.info.get('inchikey') for ligand in ligands]\n",
    "    for inchi, ligand in zip(inchikeys, ligands):\n",
    "        ligand.write(f'LigandsXYZ/{inchi}.xyz')\n",
    "    ligands_names = [get_ligand_iupacname(i) for i in inchikeys]\n",
    "    general = structure_data.get_oms()\n",
    "    oms = general.get('has_oms')\n",
    "    metal_symbols = general.get('metals')\n",
    "    metals_atomic_number = [atomic_numbers[i] for i in metal_symbols]\n",
    "    torch_data = coords_library.ase_to_pytorch_geometric(ase_data)\n",
    "    oms = torch.tensor([1 if  oms else 0], dtype=torch.int16)\n",
    "    torch_data.oms = oms\n",
    "    data['general'] = general\n",
    "    return torch_data, metals_atomic_number, inchikeys, ligands_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80a16c",
   "metadata": {},
   "source": [
    "# load cif structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b40a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "*** Open Babel Warning  in InChI code\n",
      "  #1 :Accepted unusual valence(s): Zn(1); Metal was disconnected\n",
      "==============================\n",
      "*** Open Babel Warning  in InChI code\n",
      "  #1 :Accepted unusual valence(s): Zn(1); Metal was disconnected\n",
      "==============================\n",
      "*** Open Babel Warning  in InChI code\n",
      "  #1 :Accepted unusual valence(s): O(1)\n",
      "==============================\n",
      "*** Open Babel Warning  in InChI code\n",
      "  #1 :Accepted unusual valence(s): O(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDTQOZYHNDMJCM-UHFFFAOYSA-N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/miniconda3/envs/fairmof/lib/python3.11/site-packages/pymatgen/io/cif.py:1606: FutureWarning: We strongly discourage using implicit binary/text `mode`, and this would not be allowed after 2025-06-01. I.e. you should pass t/b in `mode`.\n",
      "  with zopen(filename, mode=mode) as file:\n"
     ]
    }
   ],
   "source": [
    "torch_data, metal_atomic_number, inch, lig = load_system(\"EDUSIF.cif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "931cc249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 4], edge_index=[2, 512], edge_attr=[512, 1], lattice=[3, 3], oms=[1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612b16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0136a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data, metal_atomic_number, inch, lig = load_system(\"Zn2C43N6H29O8.cif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f30b5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "*** Open Babel Warning  in InChI code\n",
      "  #1 :Accepted unusual valence(s): O(1)\n",
      "==============================\n",
      "*** Open Babel Warning  in InChI code\n",
      "  #1 :Accepted unusual valence(s): O(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESFABUZNSNZRT-UHFFFAOYSA-N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 15:25:37,259 - INFO - 'PUGREST.NotFound: No CID found that matches the given InChI key'\n",
      "/home/antonio/miniconda3/envs/fairmof/lib/python3.11/site-packages/pymatgen/io/cif.py:1606: FutureWarning: We strongly discourage using implicit binary/text `mode`, and this would not be allowed after 2025-06-01. I.e. you should pass t/b in `mode`.\n",
      "  with zopen(filename, mode=mode) as file:\n"
     ]
    }
   ],
   "source": [
    "torch_data, metal_atomic_number, inch, lig = load_system(\"ZnC5HO5.cif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7177ee3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RESFABUZNSNZRT-UHFFFAOYSA-N']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1229a",
   "metadata": {},
   "source": [
    "# add additional info\n",
    "## atomic one-hot oms cordinates crystal_system space_group_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17791fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/miniconda3/envs/fairmof/lib/python3.11/site-packages/pymatgen/symmetry/analyzer.py:137: DeprecationWarning: dict interface is deprecated. Use attribute interface instead\n",
      "  return int(self._space_group_data[\"number\"])\n"
     ]
    }
   ],
   "source": [
    "d = torch_data\n",
    "\n",
    "# =======================\n",
    "# Parte 1: atomic one-hot\n",
    "# =======================\n",
    "node_features = d.x.numpy()\n",
    "atom_num = node_features[:, 0].astype(int)\n",
    "a, b = np.unique(atom_num, return_counts=True)\n",
    "emb = torch.zeros(120)\n",
    "for aa, bb in zip(a, b):\n",
    "    emb[aa] = bb\n",
    "d.atomic_one_hot = emb\n",
    "\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Parte 2: struttura ASE\n",
    "# =======================\n",
    "ase_atoms = pytorch_geometric_to_ase(d)\n",
    "stru = MOFstructure(ase_atoms)\n",
    "pymat = AseAtomsAdaptor.get_structure(ase_atoms)\n",
    "\n",
    "# =======================\n",
    "# Parte 3: OMS\n",
    "# =======================\n",
    "emb = torch.zeros(96)\n",
    "tmp_dict = dict()\n",
    "for i in stru.get_oms()[\"metal_info\"]:\n",
    "    cord = i[\"coordination_number\"]\n",
    "    metal = i[\"metal\"]\n",
    "\n",
    "    if metal in tmp_dict:\n",
    "        if cord > tmp_dict[metal]:\n",
    "            tmp_dict[metal] = cord\n",
    "    else:\n",
    "        tmp_dict[metal] = cord\n",
    "\n",
    "for i, j in tmp_dict.items():\n",
    "    emb[convert_metals[i]] = j\n",
    "d.cordinates = emb\n",
    "\n",
    "# =======================\n",
    "# Parte 4: spazio e sistema cristallino\n",
    "# =======================\n",
    "sga = SpacegroupAnalyzer(pymat)\n",
    "space_group_number = sga.get_space_group_number()\n",
    "emb = torch.zeros(231)\n",
    "emb[space_group_number] = 1\n",
    "d.space_group_number = emb\n",
    "\n",
    "get_crystal_system = sga.get_crystal_system()\n",
    "emb = torch.zeros(7)\n",
    "emb[convert_struct[get_crystal_system]] = 1\n",
    "d.crystal_system = emb\n",
    "# =======================\n",
    "# Parte 5: altri attributi\n",
    "# =======================\n",
    "d.oms = d.oms.view(1, 1).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20cc2364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.oms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dd312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecfcfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = []\n",
    "pred = model(d.to(device))\n",
    "for i in pred.argsort()[0]:\n",
    "    a = filetyper.category_names()[\"metal_salts\"][i.item()]\n",
    "    kept.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f182699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a file showing the names of metal_salts we removed.\n",
    "\n",
    "# create a latex table in the ESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c6d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CdBr2\n",
      "CdCl2·2.5H2O\n",
      "[Mn3(μ3-O)(O2CPh)6(py)2(H2O)]\n",
      "Ni–H4TCPP\n",
      "CuSCN\n",
      "InC20H28NO13\n",
      "K2O\n",
      "CrCl3.6H2O\n",
      "CuCl2·4H2O\n",
      "YbCl3·6H2O\n"
     ]
    }
   ],
   "source": [
    "pred = model(d.to(device))\n",
    "for i in pred.argsort()[0][0:10]:\n",
    "    a = filetyper.category_names()[\"metal_salts\"][i.item()]\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec2d23b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extras_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7de6c1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZnBr2\n",
      "ZnC34H28N6O5\n",
      "Zn(OAc)2·3H2O\n",
      "ZnEt2\n",
      "Zn(OAc)2·6H2O\n"
     ]
    }
   ],
   "source": [
    "pred = model(d.to(device))\n",
    "prx = \"Zn\"\n",
    "\n",
    "c = 0\n",
    "for i in pred.argsort()[0]:\n",
    "    a = filetyper.category_names()[\"metal_salts\"][i.item()]\n",
    "    if a[:2] == prx:\n",
    "        print(a)\n",
    "        c = c +1  \n",
    "    if c == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e96d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93279d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ad21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairmof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
