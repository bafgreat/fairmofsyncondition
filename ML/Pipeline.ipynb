{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda9bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_pyg_obj\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import fix_target_shapes,remove_unused_onehot_columns,set_seed,filter_metals\n",
    "from mofstructure import mofdeconstructor\n",
    "\n",
    "from fairmofsyncondition.read_write.coords_library import pytorch_geometric_to_ase\n",
    "\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "\n",
    "convert_struct = {'cubic':0, 'hexagonal':1, 'monoclinic':2, 'orthorhombic':3, 'tetragonal':4,'triclinic':5, 'trigonal':6}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212130",
   "metadata": {},
   "source": [
    "# GNN for Metal Salt Prediction\n",
    "\n",
    "This code implements a **Graph Neural Network (GNN)** to predict metal salts, using:\n",
    "- **Node Features**  \n",
    "- **Edge Features**  \n",
    "- **Lattice**   \n",
    "- **Oms**\n",
    "- **Atomic number**\n",
    "---\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "1. **Load the Data**  \n",
    "   Import and prepare the dataset for use in the model.\n",
    "\n",
    "2. **Define the GNN Model**  \n",
    "   Define the neural network architecture (layers, activation functions, etc.).\n",
    "\n",
    "3. **Train the Model**  \n",
    "   - Train the model on the dataset.  \n",
    "   - Save the trained GNN weights into the `tmp/` folder.  \n",
    "\n",
    "   ⚠️ **Note**:  \n",
    "   If you only want to **test the model** without re-training, you can **skip this section** and avoid running the training step.\n",
    "\n",
    "4. **Load and Evaluate the Model**  \n",
    "   - Load the trained model weights.  \n",
    "   - Evaluate the model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afb0dd",
   "metadata": {},
   "source": [
    "### 1) Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d29a2",
   "metadata": {},
   "source": [
    "\n",
    "### to get oms\n",
    "### stru.get_oms()[\"has_oms\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02499a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gnome-settings-daemon',\n",
       " 'gnome-shell',\n",
       " 'yelp',\n",
       " 'nautilus',\n",
       " 'org.gnome.TextEditor',\n",
       " 'keyrings',\n",
       " 'backgrounds',\n",
       " 'jupyter',\n",
       " 'recently-used.xbel',\n",
       " 'session_migration-ubuntu',\n",
       " 'xorg',\n",
       " 'icc',\n",
       " 'gvfs-metadata',\n",
       " 'ibus-table',\n",
       " 'flatpak',\n",
       " 'Trash',\n",
       " 'applications',\n",
       " 'nano',\n",
       " 'evolution',\n",
       " 'sounds']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17366789",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\n                An LMDB error occurred while\n                accessing the file at '../../data/mof_syncondition_data/': ../../data/mof_syncondition_data/: No such file or directory\n                ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Postdoc/Dinga/GIT/fairmofsyncondition/ML/load_data.py:1217\u001b[39m, in \u001b[36mLMDBDataset.__init__\u001b[39m\u001b[34m(self, lmdb_path)\u001b[39m\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     \u001b[38;5;28mself\u001b[39m.lmdb_env = \u001b[43mlmdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lmdb_env.begin() \u001b[38;5;28;01mas\u001b[39;00m txn:\n",
      "\u001b[31mError\u001b[39m: ../../data/mof_syncondition_data/: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m set_seed(seed=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m data_in = \u001b[43mload_pyg_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_mdb\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../data/mof_syncondition_data/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m dataset = fix_target_shapes(data_in, \u001b[33m\"\u001b[39m\u001b[33mmetal_salts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m dataset = remove_unused_onehot_columns(dataset, \u001b[33m\"\u001b[39m\u001b[33mmetal_salts\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Postdoc/Dinga/GIT/fairmofsyncondition/ML/load_data.py:35\u001b[39m, in \u001b[36mload_pyg_obj\u001b[39m\u001b[34m(path_to_mdb)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_pyg_obj\u001b[39m(path_to_mdb = \u001b[33m\"\u001b[39m\u001b[33mmof_syncondition_data\u001b[39m\u001b[33m\"\u001b[39m):    \n\u001b[32m     34\u001b[39m     data = []\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mLMDBDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_to_mdb\u001b[49m\u001b[43m)\u001b[49m:    \n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m d.metal_salts.shape[\u001b[32m0\u001b[39m] != \u001b[32m0\u001b[39m:\n\u001b[32m     37\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m d.solvents.shape[\u001b[32m0\u001b[39m] != \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Postdoc/Dinga/GIT/fairmofsyncondition/ML/load_data.py:1239\u001b[39m, in \u001b[36mLMDBDataset.__init__\u001b[39m\u001b[34m(self, lmdb_path)\u001b[39m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1233\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1234\u001b[39m \u001b[33m        ValueError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCheck if the\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[33m        LMDB file is correctly created with a\u001b[39m\n\u001b[32m   1236\u001b[39m \u001b[33m        \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__len__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m key.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1237\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mve\u001b[39;00m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m lmdb.Error \u001b[38;5;28;01mas\u001b[39;00m le:\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1241\u001b[39m \u001b[33m        An LMDB error occurred while\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[33m        accessing the file at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlmdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mle\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   1243\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1244\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mle\u001b[39;00m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1247\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[33m        An unexpected error occurred\u001b[39m\n\u001b[32m   1249\u001b[39m \u001b[33m        while initializing the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   1250\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1251\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: \n                An LMDB error occurred while\n                accessing the file at '../../data/mof_syncondition_data/': ../../data/mof_syncondition_data/: No such file or directory\n                "
     ]
    }
   ],
   "source": [
    "from mofstructure.structure import MOFstructure\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "convert_metals = {j:i for i,j in enumerate(mofdeconstructor.transition_metals()[1:])}\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(seed=42)\n",
    "\n",
    "data_in = load_pyg_obj(path_to_mdb=\"../../data/mof_syncondition_data/\")\n",
    "dataset = fix_target_shapes(data_in, \"metal_salts\")\n",
    "dataset = remove_unused_onehot_columns(dataset, \"metal_salts\")\n",
    "\n",
    "\n",
    "bad = []\n",
    "good = []\n",
    "\n",
    "if os.path.exists(\"../../dataset_cleen_all_info.pt\"):\n",
    "    print(\"Loading precomputed dataset...\")\n",
    "    dataset = torch.load(\"../../dataset_cleen_all_info.pt\")\n",
    "else:\n",
    "    print(\"Computing dataset with all info...\")\n",
    "    for d in tqdm(dataset):\n",
    "        try:\n",
    "            # =======================\n",
    "            # Parte 1: atomic one-hot\n",
    "            # =======================\n",
    "            node_features = d.x.numpy()\n",
    "            atom_num = node_features[:, 0].astype(int)\n",
    "            a, b = np.unique(atom_num, return_counts=True)\n",
    "            emb = torch.zeros(120)\n",
    "            for aa, bb in zip(a, b):\n",
    "                emb[aa] = bb\n",
    "            d.atomic_one_hot = emb\n",
    "            \n",
    "            # =======================\n",
    "            # Parte 2: struttura ASE\n",
    "            # =======================\n",
    "            ase_atoms = pytorch_geometric_to_ase(d)\n",
    "            stru = MOFstructure(ase_atoms)\n",
    "            pymat = AseAtomsAdaptor.get_structure(ase_atoms)\n",
    "            \n",
    "            # =======================\n",
    "            # Parte 3: OMS\n",
    "            # =======================\n",
    "            emb = torch.zeros(96)\n",
    "            tmp_dict = dict()\n",
    "            for i in stru.get_oms()[\"metal_info\"]:\n",
    "                cord = i[\"coordination_number\"]\n",
    "                metal = i[\"metal\"]\n",
    "\n",
    "                if metal in tmp_dict:\n",
    "                    if cord > tmp_dict[metal]:\n",
    "                        tmp_dict[metal] = cord\n",
    "                else:\n",
    "                    tmp_dict[metal] = cord\n",
    "\n",
    "            for i, j in tmp_dict.items():\n",
    "                emb[convert_metals[i]] = j\n",
    "            d.cordinates = emb\n",
    "            \n",
    "            # =======================\n",
    "            # Parte 4: spazio e sistema cristallino\n",
    "            # =======================\n",
    "            sga = SpacegroupAnalyzer(pymat)\n",
    "            space_group_number = sga.get_space_group_number()\n",
    "            emb = torch.zeros(231)\n",
    "            emb[space_group_number] = 1\n",
    "            d.space_group_number = emb\n",
    "\n",
    "            get_crystal_system = sga.get_crystal_system()\n",
    "            emb = torch.zeros(7)\n",
    "            emb[convert_struct[get_crystal_system]] = 1\n",
    "            d.crystal_system = emb\n",
    "            # =======================\n",
    "            # Parte 5: altri attributi\n",
    "            # =======================\n",
    "            d.oms = d.oms.view(1, 1).float()\n",
    "\n",
    "            ###################### no porosity is too long to compute\n",
    "            #por = stru.get_porosity()\n",
    "            #por = list(por.values())\n",
    "            #d.porosity = torch.tensor(por)\n",
    "\n",
    "            d.modified_scherrer = None\n",
    "            d.microstrain = None\n",
    "\n",
    "            # Se arrivo qui senza eccezioni → struttura buona\n",
    "            good.append(d)\n",
    "\n",
    "        except Exception:\n",
    "            bad.append(d)\n",
    "            continue\n",
    "    torch.save(good, \"../../dataset_cleen_all_info.pt\")   # salva lista di Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ec87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are classes 122 3054\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ==================== Utils & setup ====================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Filtra classi rare (come nel tuo codice)\n",
    "Y = [d.metal_salts.argmax(dim=1).item() for d in dataset]\n",
    "a,b = np.unique(Y, return_counts=True)\n",
    "conv_y = {i:j for i,j in zip(a,b)}\n",
    "good = [d for d in dataset if conv_y[d.metal_salts.argmax(dim=1).item()] > 5]\n",
    "dataset = good\n",
    "\n",
    "print(\"There are classes\", len(np.unique([d.metal_salts.argmax(dim=1).item() for d in dataset])), len(dataset))\n",
    "\n",
    "# Split & loaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=128)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=128)\n",
    "\n",
    "# ==================== NEW: ablation helpers ====================\n",
    "\n",
    "# 1) Getter per ciascuna feature extra (sempre reshaped a [B, D])\n",
    "#    Aggiungi qui dentro eventuali nuove feature future.\n",
    "\n",
    "def _reshape_feat(tensor, d):\n",
    "    # Se è Batch (ha num_graphs)\n",
    "    if hasattr(d, \"num_graphs\"):\n",
    "        return tensor.view(d.num_graphs, -1)\n",
    "    else:  # è un singolo Data\n",
    "        return tensor.view(1, -1)\n",
    "\n",
    "EXTRA_GETTERS = {\n",
    "    \"atomic_one_hot\":      lambda d: _reshape_feat(d.atomic_one_hot, d),\n",
    "    \"space_group_number\":  lambda d: _reshape_feat(d.space_group_number, d),\n",
    "    \"crystal_system\":      lambda d: _reshape_feat(d.crystal_system, d),\n",
    "    \"oms\":                 lambda d: _reshape_feat(d.oms, d),\n",
    "    \"cordinates\":          lambda d: _reshape_feat(d.cordinates, d),\n",
    "}\n",
    "def compute_extras_dim(sample_data, selected_extras):\n",
    "    dim = 0\n",
    "    for name in selected_extras:\n",
    "        if name not in EXTRA_GETTERS:\n",
    "            raise ValueError(f\"Feature extra sconosciuta: {name}\")\n",
    "        dim += EXTRA_GETTERS[name](sample_data).shape[1]\n",
    "    return dim\n",
    "\n",
    "def build_extras_tensor(data, selected_extras):\n",
    "    if not selected_extras:\n",
    "        return None\n",
    "    parts = [EXTRA_GETTERS[name](data) for name in selected_extras]\n",
    "    return torch.cat(parts, dim=1)\n",
    "\n",
    "def extras_suffix(selected_extras):\n",
    "    if not selected_extras:\n",
    "        return \"no_extras\"\n",
    "    return \"_\".join(selected_extras)\n",
    "\n",
    "# ==================== Model ====================\n",
    "\n",
    "class MetalSaltGNN_Ablation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim,\n",
    "        edge_in_dim,\n",
    "        lattice_in_dim=9,\n",
    "        hidden_dim=128,\n",
    "        num_classes=10,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=None,      # NEW: lista di nomi feature extra\n",
    "        extras_dim=0               # NEW: dimensione totale delle extra\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout = dropout\n",
    "        self.selected_extras = selected_extras or []\n",
    "        self.extras_dim = extras_dim\n",
    "\n",
    "        # --- Edge encoder (per GINE)\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # --- GINE layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_bns = nn.ModuleList() if use_batchnorm else None\n",
    "        for i in range(num_gnn_layers):\n",
    "            in_dim = node_in_dim if i == 0 else hidden_dim\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.gnn_layers.append(GINEConv(mlp, edge_dim=hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.gnn_bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # --- Lattice encoder\n",
    "        lattice_layers = []\n",
    "        in_dim = lattice_in_dim\n",
    "        for _ in range(max(1, num_lattice_layers - 1)):\n",
    "            lattice_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                lattice_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "        lattice_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        self.lattice_encoder = nn.Sequential(*lattice_layers)\n",
    "\n",
    "        # --- Final MLP head\n",
    "        final_in = hidden_dim * 2 + self.extras_dim  # graph pooled + lattice + extras\n",
    "        mlp_layers = []\n",
    "        in_dim = final_in\n",
    "        for _ in range(max(1, num_mlp_layers - 1)):\n",
    "            mlp_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                mlp_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            mlp_layers.append(nn.Dropout(p=dropout))\n",
    "            in_dim = hidden_dim\n",
    "        mlp_layers.append(nn.Linear(in_dim, num_classes))\n",
    "        self.final_mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode edges\n",
    "        e = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # GNN layers\n",
    "        for i, conv in enumerate(self.gnn_layers):\n",
    "            x = conv(x, edge_index, e)\n",
    "            x = F.relu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.gnn_bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        x_pool = global_mean_pool(x, batch)\n",
    "\n",
    "        # Lattice encoding (sempre usato)\n",
    "        lattice = data.lattice.view(-1, 9)\n",
    "        lattice_feat = self.lattice_encoder(lattice)\n",
    "\n",
    "        # Extras (abilitate in base alla lista)\n",
    "        extras = build_extras_tensor(data, self.selected_extras)\n",
    "        if extras is not None:\n",
    "            final_in = torch.cat([x_pool, lattice_feat, extras], dim=1)\n",
    "        else:\n",
    "            final_in = torch.cat([x_pool, lattice_feat], dim=1)\n",
    "\n",
    "        out = self.final_mlp(final_in)\n",
    "        return out\n",
    "\n",
    "# ==================== Train / Eval helpers ====================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, target_name):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = torch.argmax(data[target_name], dim=1).long()\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, target_name):\n",
    "    model.eval()\n",
    "    correct = {1: 0, 3: 0, 5: 0, 10: 0}\n",
    "    total = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data)\n",
    "        labels = torch.argmax(data[target_name], dim=1).long()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        _, pred = logits.topk(10, dim=1)\n",
    "        for k in correct.keys():\n",
    "            correct[k] += (pred[:, :k] == labels.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "        top1_preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(top1_preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    macro_f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average=\"macro\")\n",
    "\n",
    "    results = {f\"top{k}_acc\": correct[k] / max(1, total) for k in correct}\n",
    "    results[\"macro_f1\"] = macro_f1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b26659",
   "metadata": {},
   "source": [
    "### 2) Define the GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b59c9e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atomic_one_hot', 'cordinates', 'oms', 'space_group_number']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==================== Run ablation ====================\n",
    "\n",
    "node_in_dim = dataset[0].x.shape[1]\n",
    "edge_in_dim = dataset[0].edge_attr.shape[1]\n",
    "lattice_in_dim = 9\n",
    "\n",
    "# Scegli qui le extra da usare nell’ablation:\n",
    "# Esempio richiesto: [\"oms\", \"atomic_one_hot\"]\n",
    "\n",
    "\n",
    "EXTRA_GETTERS = {\n",
    "    \"atomic_one_hot\":      lambda d: _reshape_feat(d.atomic_one_hot, d),\n",
    "    \"space_group_number\":  lambda d: _reshape_feat(d.space_group_number, d),\n",
    "    \"crystal_system\":      lambda d: _reshape_feat(d.crystal_system, d),\n",
    "    \"oms\":                 lambda d: _reshape_feat(d.oms, d),\n",
    "    \"cordinates\":          lambda d: _reshape_feat(d.cordinates, d),\n",
    "}\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\", \"cordinates\",\"crystal_system\", \"oms\",\"space_group_number\"]\n",
    "selected_extras = []\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\"]\n",
    "selected_extras = [\"cordinates\"]\n",
    "selected_extras = [\"crystal_system\"]\n",
    "selected_extras = [\"oms\"]\n",
    "selected_extras = [\"space_group_number\"]\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\",\"cordinates\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"crystal_system\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"space_group_number\"]\n",
    "\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"cordinates\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"crystal_system\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"space_group_number\"]\n",
    "\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"cordinates\", \"crystal_system\"]\n",
    "selected_extras = [\"atomic_one_hot\", \"oms\", \"cordinates\", \"space_group_number\"]\n",
    "# imoprtant: for saving\n",
    "selected_extras = np.sort(selected_extras).tolist()\n",
    "selected_extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0139d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training config: HID64_DO0.35_SEED1__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4525, top5_acc=0.6984, top3_acc=0.6328 macro_f1=0.1590\n",
      "VAL: top1_acc=0.4984, top5_acc=0.7738, top3_acc=0.6918 macro_f1=0.1921\n",
      "VAL: top1_acc=0.4984, top5_acc=0.7902, top3_acc=0.7016 macro_f1=0.1935\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8164, top3_acc=0.7082 macro_f1=0.2104\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8066, top3_acc=0.6984 macro_f1=0.2369\n",
      "VAL: top1_acc=0.5180, top5_acc=0.7967, top3_acc=0.6984 macro_f1=0.2564\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2476\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8098, top3_acc=0.7246 macro_f1=0.2424\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8131, top3_acc=0.7213 macro_f1=0.2518\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8230, top3_acc=0.7246 macro_f1=0.2398\n",
      "VAL: top1_acc=0.5377, top5_acc=0.8262, top3_acc=0.7311 macro_f1=0.2573\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8066, top3_acc=0.7082 macro_f1=0.2572\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8197, top3_acc=0.7180 macro_f1=0.2534\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8426, top3_acc=0.7344 macro_f1=0.2668\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8230, top3_acc=0.7410 macro_f1=0.2337\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8131, top3_acc=0.7213 macro_f1=0.2839\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8131, top3_acc=0.7180 macro_f1=0.2903\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8197, top3_acc=0.7344 macro_f1=0.2847\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8033, top3_acc=0.7148 macro_f1=0.2702\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8033, top3_acc=0.7311 macro_f1=0.2642\n",
      "VAL: top1_acc=0.5344, top5_acc=0.8066, top3_acc=0.7344 macro_f1=0.2942\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8164, top3_acc=0.7148 macro_f1=0.2994\n",
      "VAL: top1_acc=0.5377, top5_acc=0.8164, top3_acc=0.7311 macro_f1=0.3145\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8131, top3_acc=0.7115 macro_f1=0.2946\n",
      "Early stopping at epoch 120\n",
      "HID64_DO0.35_SEED1__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4739, top5_acc=0.7745, top3_acc=0.6601 macro_f1=0.2989\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED2__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4295, top5_acc=0.7213, top3_acc=0.6197 macro_f1=0.0986\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7607, top3_acc=0.6557 macro_f1=0.1765\n",
      "VAL: top1_acc=0.4918, top5_acc=0.7639, top3_acc=0.6754 macro_f1=0.1857\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7869, top3_acc=0.6984 macro_f1=0.2348\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8000, top3_acc=0.6918 macro_f1=0.2202\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8131, top3_acc=0.6984 macro_f1=0.2508\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2114\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2187\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8328, top3_acc=0.7115 macro_f1=0.2362\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8262, top3_acc=0.7344 macro_f1=0.2209\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8328, top3_acc=0.7279 macro_f1=0.2454\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8295, top3_acc=0.7246 macro_f1=0.2555\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8393, top3_acc=0.7279 macro_f1=0.2353\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8295, top3_acc=0.7246 macro_f1=0.2229\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8328, top3_acc=0.7148 macro_f1=0.2153\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8393, top3_acc=0.7475 macro_f1=0.2554\n",
      "VAL: top1_acc=0.5344, top5_acc=0.8361, top3_acc=0.7475 macro_f1=0.2561\n",
      "VAL: top1_acc=0.5410, top5_acc=0.8328, top3_acc=0.7377 macro_f1=0.2636\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8361, top3_acc=0.7148 macro_f1=0.2402\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8131, top3_acc=0.7279 macro_f1=0.2462\n",
      "VAL: top1_acc=0.5410, top5_acc=0.8393, top3_acc=0.7377 macro_f1=0.2841\n",
      "VAL: top1_acc=0.5246, top5_acc=0.8295, top3_acc=0.7410 macro_f1=0.2848\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8361, top3_acc=0.7279 macro_f1=0.2580\n",
      "Early stopping at epoch 115\n",
      "HID64_DO0.35_SEED2__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4641, top5_acc=0.7908, top3_acc=0.6961 macro_f1=0.2821\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED3__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4459, top5_acc=0.7148, top3_acc=0.6426 macro_f1=0.1219\n",
      "VAL: top1_acc=0.5016, top5_acc=0.7672, top3_acc=0.6820 macro_f1=0.1890\n",
      "VAL: top1_acc=0.5115, top5_acc=0.7934, top3_acc=0.6984 macro_f1=0.2115\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8164, top3_acc=0.7180 macro_f1=0.2320\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8098, top3_acc=0.7213 macro_f1=0.2274\n",
      "VAL: top1_acc=0.5311, top5_acc=0.7934, top3_acc=0.7246 macro_f1=0.2316\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8164, top3_acc=0.7246 macro_f1=0.2488\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8131, top3_acc=0.7016 macro_f1=0.2249\n",
      "VAL: top1_acc=0.5377, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2524\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8033, top3_acc=0.6984 macro_f1=0.2512\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8066, top3_acc=0.7180 macro_f1=0.2654\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8197, top3_acc=0.7115 macro_f1=0.2543\n",
      "VAL: top1_acc=0.5311, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2833\n",
      "VAL: top1_acc=0.5311, top5_acc=0.7836, top3_acc=0.7016 macro_f1=0.2560\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8098, top3_acc=0.7082 macro_f1=0.2492\n",
      "VAL: top1_acc=0.5344, top5_acc=0.8361, top3_acc=0.7377 macro_f1=0.2892\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8131, top3_acc=0.7410 macro_f1=0.2648\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8131, top3_acc=0.6885 macro_f1=0.2219\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8131, top3_acc=0.7213 macro_f1=0.2321\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8098, top3_acc=0.7213 macro_f1=0.2426\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8066, top3_acc=0.7082 macro_f1=0.2603\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7967, top3_acc=0.6918 macro_f1=0.2564\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8164, top3_acc=0.7279 macro_f1=0.2584\n",
      "VAL: top1_acc=0.4820, top5_acc=0.8197, top3_acc=0.7213 macro_f1=0.2277\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2579\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8066, top3_acc=0.7049 macro_f1=0.2349\n",
      "Early stopping at epoch 130\n",
      "HID64_DO0.35_SEED3__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4771, top5_acc=0.7680, top3_acc=0.7092 macro_f1=0.2970\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED4__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4689, top5_acc=0.6918, top3_acc=0.6164 macro_f1=0.1345\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7738, top3_acc=0.6689 macro_f1=0.1767\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7803, top3_acc=0.6885 macro_f1=0.1869\n",
      "VAL: top1_acc=0.5049, top5_acc=0.7967, top3_acc=0.6951 macro_f1=0.1940\n",
      "VAL: top1_acc=0.5115, top5_acc=0.7967, top3_acc=0.6918 macro_f1=0.2045\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8033, top3_acc=0.7115 macro_f1=0.2212\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7902, top3_acc=0.7016 macro_f1=0.2126\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8098, top3_acc=0.6885 macro_f1=0.2255\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8197, top3_acc=0.6951 macro_f1=0.2226\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8230, top3_acc=0.7443 macro_f1=0.2421\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8262, top3_acc=0.7180 macro_f1=0.2329\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8098, top3_acc=0.7148 macro_f1=0.2187\n",
      "VAL: top1_acc=0.5082, top5_acc=0.7967, top3_acc=0.6852 macro_f1=0.2360\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8328, top3_acc=0.7016 macro_f1=0.2465\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8131, top3_acc=0.7180 macro_f1=0.2293\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8426, top3_acc=0.7377 macro_f1=0.2781\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8098, top3_acc=0.7148 macro_f1=0.2139\n",
      "VAL: top1_acc=0.5279, top5_acc=0.8361, top3_acc=0.7541 macro_f1=0.2609\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8230, top3_acc=0.7279 macro_f1=0.2553\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8230, top3_acc=0.7443 macro_f1=0.2492\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8361, top3_acc=0.7508 macro_f1=0.2549\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8098, top3_acc=0.7148 macro_f1=0.2550\n",
      "VAL: top1_acc=0.4951, top5_acc=0.8164, top3_acc=0.7311 macro_f1=0.2576\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8426, top3_acc=0.7377 macro_f1=0.2581\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8295, top3_acc=0.7279 macro_f1=0.2898\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8328, top3_acc=0.7246 macro_f1=0.2644\n",
      "Early stopping at epoch 130\n",
      "HID64_DO0.35_SEED4__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4608, top5_acc=0.7745, top3_acc=0.6699 macro_f1=0.2875\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED5__atomic_one_hot_cordinates_oms_space_group_number =====\n",
      "VAL: top1_acc=0.4393, top5_acc=0.7016, top3_acc=0.6295 macro_f1=0.1284\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7574, top3_acc=0.6557 macro_f1=0.1799\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7639, top3_acc=0.6787 macro_f1=0.1963\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7934, top3_acc=0.6984 macro_f1=0.2194\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7869, top3_acc=0.7148 macro_f1=0.2531\n",
      "VAL: top1_acc=0.5148, top5_acc=0.7967, top3_acc=0.6820 macro_f1=0.2410\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8033, top3_acc=0.7049 macro_f1=0.2308\n",
      "VAL: top1_acc=0.5410, top5_acc=0.8066, top3_acc=0.7115 macro_f1=0.2863\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8197, top3_acc=0.7180 macro_f1=0.2253\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8098, top3_acc=0.7049 macro_f1=0.2316\n",
      "VAL: top1_acc=0.5148, top5_acc=0.8164, top3_acc=0.7148 macro_f1=0.2605\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8066, top3_acc=0.7213 macro_f1=0.2600\n",
      "VAL: top1_acc=0.4984, top5_acc=0.8262, top3_acc=0.7246 macro_f1=0.2372\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8328, top3_acc=0.7213 macro_f1=0.2555\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8098, top3_acc=0.7344 macro_f1=0.2458\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8328, top3_acc=0.7344 macro_f1=0.2776\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8131, top3_acc=0.7049 macro_f1=0.2598\n",
      "VAL: top1_acc=0.4721, top5_acc=0.8066, top3_acc=0.7344 macro_f1=0.2528\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8361, top3_acc=0.7180 macro_f1=0.2696\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8164, top3_acc=0.7213 macro_f1=0.2562\n",
      "VAL: top1_acc=0.5016, top5_acc=0.8197, top3_acc=0.6984 macro_f1=0.2598\n",
      "VAL: top1_acc=0.4820, top5_acc=0.8230, top3_acc=0.7180 macro_f1=0.2650\n",
      "VAL: top1_acc=0.5180, top5_acc=0.8066, top3_acc=0.7344 macro_f1=0.2798\n",
      "VAL: top1_acc=0.4951, top5_acc=0.8131, top3_acc=0.7180 macro_f1=0.2379\n",
      "VAL: top1_acc=0.5213, top5_acc=0.8361, top3_acc=0.7148 macro_f1=0.2723\n",
      "VAL: top1_acc=0.4885, top5_acc=0.8164, top3_acc=0.7049 macro_f1=0.2713\n",
      "VAL: top1_acc=0.5082, top5_acc=0.8295, top3_acc=0.7082 macro_f1=0.2700\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8393, top3_acc=0.7311 macro_f1=0.2505\n",
      "VAL: top1_acc=0.4885, top5_acc=0.8131, top3_acc=0.7148 macro_f1=0.2465\n",
      "VAL: top1_acc=0.4361, top5_acc=0.8230, top3_acc=0.7148 macro_f1=0.2507\n",
      "VAL: top1_acc=0.5115, top5_acc=0.8197, top3_acc=0.7148 macro_f1=0.2813\n",
      "VAL: top1_acc=0.5049, top5_acc=0.8164, top3_acc=0.7344 macro_f1=0.2892\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8131, top3_acc=0.7115 macro_f1=0.2577\n",
      "VAL: top1_acc=0.5016, top5_acc=0.7967, top3_acc=0.7049 macro_f1=0.2437\n",
      "VAL: top1_acc=0.4918, top5_acc=0.8328, top3_acc=0.7279 macro_f1=0.2552\n",
      "VAL: top1_acc=0.4787, top5_acc=0.8000, top3_acc=0.6820 macro_f1=0.2626\n",
      "VAL: top1_acc=0.4885, top5_acc=0.8131, top3_acc=0.6885 macro_f1=0.2805\n",
      "VAL: top1_acc=0.4951, top5_acc=0.7869, top3_acc=0.6951 macro_f1=0.2974\n",
      "Early stopping at epoch 190\n",
      "HID64_DO0.35_SEED5__atomic_one_hot_cordinates_oms_space_group_number TEST: top1_acc=0.4510, top5_acc=0.7908, top3_acc=0.6895 macro_f1=0.2857\n"
     ]
    }
   ],
   "source": [
    "# Calcolo dinamico della dimensione delle extra\n",
    "extras_dim = compute_extras_dim(dataset[0], selected_extras)\n",
    "\n",
    "# Classi\n",
    "Y_size = max([torch.argmax(d[\"metal_salts\"]).item() for d in dataset])\n",
    "num_classes = Y_size + 1\n",
    "\n",
    "number_of_runs = [1,2,3,4,5]\n",
    "hidden_dim = 64\n",
    "dropout = 0.35\n",
    "\n",
    "results = []\n",
    "suffix = extras_suffix(selected_extras)\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__{suffix}\"\n",
    "    print(f\"\\n===== Training config: {config_name} =====\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = MetalSaltGNN_Ablation(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=selected_extras,\n",
    "        extras_dim=extras_dim\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    checkpoint_name = f\"tmp2/Metal_salts_{config_name}.pt\"\n",
    "\n",
    "    best_metric = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50\n",
    "    eval_every = 5\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        _ = train_one_epoch(model, train_loader, criterion, optimizer, device, \"metal_salts\")\n",
    "        if epoch % eval_every == 0:\n",
    "            res = evaluate(model, val_loader, device, \"metal_salts\")\n",
    "            print(f\"VAL: top1_acc={res['top1_acc']:.4f}, top5_acc={res['top5_acc']:.4f}, top3_acc={res['top3_acc']:.4f} macro_f1={res['macro_f1']:.4f}\")\n",
    "\n",
    "            # Early stopping su top5, come nel tuo codice\n",
    "            if res[\"top5_acc\"] > best_metric:\n",
    "                best_metric = res[\"top5_acc\"]\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), checkpoint_name)\n",
    "            else:\n",
    "                epochs_no_improve += eval_every\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # Valutazione test con il best checkpoint\n",
    "    model.load_state_dict(torch.load(checkpoint_name, map_location=device))\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "    results.append({**res_test, 'config': config_name})\n",
    "    print(f\"{config_name} TEST: top1_acc={res_test['top1_acc']:.4f}, top5_acc={res_test['top5_acc']:.4f}, top3_acc={res_test['top3_acc']:.4f} macro_f1={res_test['macro_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d617444",
   "metadata": {},
   "source": [
    "# evalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2ee7807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Media ± Std sulle run ====\n",
      "\n",
      "top1_acc: 0.4654 ± 0.0094\n",
      "top3_acc: 0.6850 ± 0.0177\n",
      "top5_acc: 0.7797 ± 0.0094\n",
      "top10_acc: 0.8993 ± 0.0084\n",
      "macro_f1: 0.2902 ± 0.0066\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__{suffix}\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    model = MetalSaltGNN_Ablation(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=selected_extras,\n",
    "        extras_dim=extras_dim\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint_name = f\"tmp2/Metal_salts_{config_name}.pt\"\n",
    "    checkpoint = torch.load(checkpoint_name, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "\n",
    "    results.append({\n",
    "        'config': config_name,\n",
    "        'top1_acc':  res_test['top1_acc'],\n",
    "        'top3_acc':  res_test['top3_acc'],\n",
    "        'top5_acc':  res_test['top5_acc'],\n",
    "        'top10_acc': res_test['top10_acc'],\n",
    "        'macro_f1':  res_test['macro_f1'],\n",
    "    })\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "# results è la tua lista di dict\n",
    "metrics = [\"top1_acc\",\"top3_acc\",\"top5_acc\",\"top10_acc\",\"macro_f1\"]\n",
    "\n",
    "print(\"==== Media ± Std sulle run ====\\n\")\n",
    "for metric in metrics:\n",
    "    values = [r[metric] for r in results]\n",
    "    mean = np.mean(values)\n",
    "    std  = np.std(values)\n",
    "    print(f\"{metric}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230c529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2d298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5483cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991cdbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169c3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7be2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool,GINConv\n",
    "\n",
    "\n",
    "# ==================== Model ====================\n",
    "\n",
    "class MetalSaltGNN_v2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim=4,     # da x.shape[1]\n",
    "        edge_in_dim=1,     # da edge_attr.shape[1]\n",
    "        lattice_in_dim=9,  # 3x3 flatten\n",
    "        hidden_dim=128,\n",
    "        num_classes=10,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # --- Edge encoder (per GINE)\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # --- GINE layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_bns = nn.ModuleList() if use_batchnorm else None\n",
    "        for i in range(num_gnn_layers):\n",
    "            in_dim = node_in_dim if i == 0 else hidden_dim\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.gnn_layers.append(GINEConv(mlp, edge_dim=hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.gnn_bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # --- Lattice encoder\n",
    "        lattice_layers = []\n",
    "        in_dim = lattice_in_dim\n",
    "        for _ in range(max(1, num_lattice_layers - 1)):\n",
    "            lattice_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                lattice_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "        lattice_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        self.lattice_encoder = nn.Sequential(*lattice_layers)\n",
    "\n",
    "        # --- Final MLP head\n",
    "        # Extra graph-level features\n",
    "        extra_dim = 120 + 231 + 7 + 1 + 8 + 96\n",
    "        final_in = hidden_dim * 2 + extra_dim\n",
    "\n",
    "        mlp_layers = []\n",
    "        in_dim = final_in\n",
    "        for _ in range(max(1, num_mlp_layers - 1)):\n",
    "            mlp_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                mlp_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            mlp_layers.append(nn.Dropout(p=dropout))\n",
    "            in_dim = hidden_dim\n",
    "        mlp_layers.append(nn.Linear(in_dim, num_classes))\n",
    "        self.final_mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        lattice = data.lattice\n",
    "\n",
    "        # Per-graph extras\n",
    "        atomic_oh = data.atomic_one_hot.view(-1, 120)\n",
    "        sg_oh = data.space_group_number.view(-1, 231)\n",
    "        cs_oh = data.crystal_system.view(-1, 7)\n",
    "        oms = data.oms.view(-1, 1)\n",
    "        coords = data.cordinates.view(-1, 96)\n",
    "\n",
    "        # Encode edges\n",
    "        e = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # GNN layers\n",
    "        for i, conv in enumerate(self.gnn_layers):\n",
    "            x = conv(x, edge_index, e)\n",
    "            x = F.relu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.gnn_bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        x_pool = global_mean_pool(x, batch)\n",
    "\n",
    "        # Lattice encoding\n",
    "        lattice = lattice.view(-1, 9)\n",
    "        lattice_feat = self.lattice_encoder(lattice)\n",
    "\n",
    "        # Concatenate everything\n",
    "        extras = torch.cat([atomic_oh, sg_oh, cs_oh, oms, coords], dim=1)\n",
    "        final_in = torch.cat([x_pool, lattice_feat, extras], dim=1)\n",
    "\n",
    "        out = self.final_mlp(final_in)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ==================== Train / Eval helpers ====================\n",
    "def train(model, loader, criterion, optimizer, device, target_name):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # target expected one-hot per-graph -> class indices\n",
    "        target = torch.argmax(data[target_name], dim=1).long()\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, target_name):\n",
    "    model.eval()\n",
    "    correct = {1: 0, 3: 0, 5: 0, 10: 0}\n",
    "    total = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data)\n",
    "        labels = torch.argmax(data[target_name], dim=1).long()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Top-k accuracy\n",
    "        _, pred = logits.topk(10, dim=1)\n",
    "        for k in correct.keys():\n",
    "            correct[k] += (pred[:, :k] == labels.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "        # Per F1: usiamo solo la predizione top1\n",
    "        top1_preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(top1_preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatena\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    macro_f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average=\"macro\")\n",
    "\n",
    "    results = {f\"top{k}_acc\": correct[k] / max(1, total) for k in correct}\n",
    "    results[\"macro_f1\"] = macro_f1\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_in_dim = dataset[0].x.shape[1]\n",
    "edge_in_dim = dataset[0].edge_attr.shape[1]\n",
    "lattice_in_dim = 9\n",
    "\n",
    "number_of_runs = [1,2,3]  # \n",
    "\n",
    "hidden_dim = 64\n",
    "dropout = 0.35\n",
    "\n",
    "\n",
    "Y_size = max([torch.argmax(d[\"metal_salts\"]).item() for d in dataset])\n",
    "num_classes = Y_size + 1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a12f7b",
   "metadata": {},
   "source": [
    "### 3) Train the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__X_edgeAttr_lattice_oms_AtomicNumber_structsym\"\n",
    "    print(f\"\\n===== Training config: {config_name} =====\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # ==================== Model ====================\n",
    "    model = MetalSaltGNN_v2(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes, \n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay=0.0001)\n",
    "    checkpoint_name = f\"tmp2/Metal_salts_{config_name}.pt\"\n",
    "    best_metric = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50\n",
    "    eval_every = 5\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        train(model, train_loader, criterion, optimizer, device, \"metal_salts\")\n",
    "        if epoch % eval_every == 0:\n",
    "            res = evaluate(model, val_loader, device, \"metal_salts\")\n",
    "            print(f\"VAL: top1_acc={res['top1_acc']:.4f}, top5_acc={res['top5_acc']:.4f}, top3_acc={res['top3_acc']:.4f} macro_f1={res['macro_f1']:.4f}\")\n",
    "            \n",
    "            if res[\"top5_acc\"] > best_metric:\n",
    "                best_metric = res[\"top5_acc\"]\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), checkpoint_name)\n",
    "            else:\n",
    "                epochs_no_improve += eval_every\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_name))\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "    results.append({**res_test, 'config': config_name})\n",
    "    print(f\"{config_name} TEST: top1_acc={res_test['top1_acc']:.4f}, top5_acc={res_test['top5_acc']:.4f}, top3_acc={res_test['top3_acc']:.4f} macro_f1={res_test['macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25722051",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db04f4",
   "metadata": {},
   "source": [
    "### 4) Load and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22757017",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__X_edgeAttr_lattice_oms_AtomicNumber_structsym\"\n",
    "    print(f\"\\n===== Evaluating config: {config_name} =====\")\n",
    "    \n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    model = MetalSaltGNN_v2(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes, \n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint_name = f\"tmp2/Metal_salts_{config_name}.pt\"\n",
    "    # Carica best model e valuta su test\n",
    "    checkpoint = torch.load(checkpoint_name, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "\n",
    "    results.append({\n",
    "        'config': config_name,\n",
    "        'top1_acc': res_test['top1_acc'],\n",
    "        'top10_acc': res_test['top10_acc'],\n",
    "        'top5_acc': res_test['top5_acc'],\n",
    "        'top3_acc': res_test['top3_acc'],\n",
    "        'macro_f1': res_test['macro_f1']\n",
    "    })\n",
    "    print(f\"{config_name} TEST: top10_acc={res_test['top10_acc']:.4f}, top5_acc={res_test['top5_acc']:.4f}, top3_acc={res_test['top3_acc']:.4f}, macro_f1={res_test['macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f6466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bde3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416cc751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6580e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(seed=42)\n",
    "\n",
    "data_in = load_pyg_obj(path_to_mdb=\"../../data/mof_syncondition_data/\")\n",
    "dataset = fix_target_shapes(data_in, \"metal_salts\")\n",
    "dataset = remove_unused_onehot_columns(dataset, \"metal_salts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d990e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf292fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.nn.functional.one_hot(torch.tensor(idx), num_classes=len(unique_atomic_numbers)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed39174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e476ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7bf471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f2a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375f232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884932c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7852b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa08e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55aa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba51402",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3fa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(atom_num,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71524153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[0,8,0,0,0,..,32,0,4,0,0,0,0,0,0] # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc6622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.data import atomic_numbers\n",
    "\n",
    "atomic_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752aa486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc224fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a, _ = np.unique(filter_metals(d.x.numpy()[:,0].astype(int)), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f77c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metals(d.x.numpy()[:,0].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mofstructure.mofdeconstructor import transition_metals\n",
    "\n",
    "\n",
    "from ase.data import atomic_numbers\n",
    "\n",
    "len(transition_metals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.data import atomic_numbers\n",
    "\n",
    "atomic_numbers\n",
    "\n",
    "\n",
    "[atomic_numbers[i] for i in transition_metals()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86a05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18032669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_atomic_numbers = np.unique(atomic_numb)\n",
    "atomic_to_idx = {num: idx for idx, num in enumerate(unique_atomic_numbers)}\n",
    "\n",
    "for data in dataset:\n",
    "    node_features = data.x.numpy()\n",
    "    atomic_number = filter_metals(node_features[:, 0].astype(int))\n",
    "    atomic_number = np.unique(atomic_number)[0]\n",
    "    idx = atomic_to_idx[atomic_number]\n",
    "    one_hot = torch.nn.functional.one_hot(torch.tensor(idx), num_classes=len(unique_atomic_numbers)).float()\n",
    "    data.atomic_one_hot = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.data import chemical_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293cd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemical_symbols[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d770a8",
   "metadata": {},
   "source": [
    "# real word experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19c260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96ff61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b92e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mofstructure import structure, mofdeconstructor\n",
    "from mofstructure.filetyper import load_iupac_names\n",
    "from fairmofsyncondition.read_write import cheminfo2iupac, coords_library, filetyper\n",
    "from ase.data import atomic_numbers\n",
    "from ase.io import read\n",
    "import torch\n",
    "\n",
    "\n",
    "inchi_corrector = {\n",
    "    \"FDTQOZYHNDMJCM-UHFFFAOYSA-N\":\"benzene-1,4-dicarboxylic acid\"\n",
    "}\n",
    "def get_ligand_iupacname(ligand_inchi):\n",
    "    print(ligand_inchi)\n",
    "    name = load_iupac_names().get(ligand_inchi, None)\n",
    "    if name is None:\n",
    "        pubchem = cheminfo2iupac.pubchem_to_inchikey(ligand_inchi, name='inchikey')\n",
    "        if pubchem is None:\n",
    "            name = inchi_corrector.get(ligand_inchi, ligand_inchi)\n",
    "        else:\n",
    "            pubchem.get('iupac_name', ligand_inchi)\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_system(filename):\n",
    "    \"\"\"\n",
    "    A function to extract\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    os.makedirs('LigandsXYZ', exist_ok=True)\n",
    "    ase_data = read(filename)\n",
    "    structure_data = structure.MOFstructure(ase_atoms=ase_data)\n",
    "    _, ligands = structure_data.get_ligands()\n",
    "\n",
    "    inchikeys = [ligand.info.get('inchikey') for ligand in ligands]\n",
    "    for inchi, ligand in zip(inchikeys, ligands):\n",
    "        ligand.write(f'LigandsXYZ/{inchi}.xyz')\n",
    "    ligands_names = [get_ligand_iupacname(i) for i in inchikeys]\n",
    "    general = structure_data.get_oms()\n",
    "    oms = general.get('has_oms')\n",
    "    metal_symbols = general.get('metals')\n",
    "    metals_atomic_number = [atomic_numbers[i] for i in metal_symbols]\n",
    "    torch_data = coords_library.ase_to_pytorch_geometric(ase_data)\n",
    "    oms = torch.tensor([1 if  oms else 0], dtype=torch.int16)\n",
    "    torch_data.oms = oms\n",
    "    data['general'] = general\n",
    "    return torch_data, metals_atomic_number, inchikeys, ligands_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b40a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data, metal_atomic_number, inch, lig = load_system(\"EDUSIF.cif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data, metal_atomic_number, inch, lig = load_system(\"Zn2C43N6H29O8.cif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data, metal_atomic_number, inch, lig = load_system(\"ZnC5HO5.cif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17791fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(d.to(device))\n",
    "prx = \"Zn\"\n",
    "\n",
    "c = 0\n",
    "for i in pred.argsort()[0]:\n",
    "    a = filetyper.category_names()[\"metal_salts\"][i.item()]\n",
    "    \n",
    "    if a[:2] == prx:\n",
    "        print(a)\n",
    "        c = c +1 \n",
    "        \n",
    "    if c == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e96d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93279d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ad21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairmof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
