{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda9bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/miniconda3/envs/fairmof/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_pyg_obj\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils import fix_target_shapes,remove_unused_onehot_columns,set_seed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212130",
   "metadata": {},
   "source": [
    "# GNN for Metal Salt Prediction\n",
    "\n",
    "\n",
    "This code implements a **Graph Neural Network (GNN)** to predict metal salts, using:\n",
    "- **Node Features**  \n",
    "- **Edge Features**  \n",
    "- **Lattice**  \n",
    "- **Modified scherrer**\n",
    "- **Microstrain**\n",
    "- **Oms**\n",
    "\n",
    "---\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "1. **Load the Data**  \n",
    "   Import and prepare the dataset for use in the model.\n",
    "\n",
    "2. **Define the GNN Model**  \n",
    "   Define the neural network architecture (layers, activation functions, etc.).\n",
    "\n",
    "3. **Train the Model**  \n",
    "   - Train the model on the dataset.  \n",
    "   - Save the trained GNN weights into the `tmp/` folder.  \n",
    "\n",
    "   ⚠️ **Note**:  \n",
    "   If you only want to **test the model** without re-training, you can **skip this section** and avoid running the training step.\n",
    "\n",
    "4. **Load and Evaluate the Model**  \n",
    "   - Load the trained model weights.  \n",
    "   - Evaluate the model performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afb0dd",
   "metadata": {},
   "source": [
    "### 1) Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35b9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset. Note the dataset is not included in the git repo, you have to download it!\n",
    "data_in = load_pyg_obj(path_to_mdb=\"../../data/mof_syncondition_data/\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "#  Train / Validation / Test Split (80/10/10)\n",
    "dataset = fix_target_shapes(data_in,\"metal_salts\")\n",
    "dataset = remove_unused_onehot_columns(dataset,\"metal_salts\")\n",
    "Y_size = max([torch.argmax(d[\"metal_salts\"]).item() for d in dataset])\n",
    "set_seed(seed=42) # 42\n",
    "num_classes = Y_size+1\n",
    "input_dim = dataset[0].x.shape[1]\n",
    "\n",
    "\n",
    "# Add and reshape extra features\n",
    "for data in dataset:\n",
    "    data.modified_scherrer = data.modified_scherrer.view(1, 1).float()\n",
    "    data.microstrain = data.microstrain.view(1, 1).float()\n",
    "    data.oms = data.oms.view(1, 1).float()\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b26659",
   "metadata": {},
   "source": [
    "### 2) Define the GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7be2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "### GNN architecture \n",
    "\n",
    "# ==================== Modello ====================\n",
    "class MetalSaltGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim,\n",
    "        edge_in_dim,\n",
    "        lattice_in_dim,\n",
    "        extra_feat_dim,\n",
    "        hidden_dim,\n",
    "        num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_bns = nn.ModuleList() if use_batchnorm else None\n",
    "\n",
    "        for i in range(num_gnn_layers):\n",
    "            in_dim = node_in_dim if i == 0 else hidden_dim\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            )\n",
    "            self.gnn_layers.append(GINEConv(mlp, edge_dim=hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.gnn_bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Lattice encoder\n",
    "        lattice_layers = []\n",
    "        in_dim = lattice_in_dim\n",
    "        for _ in range(num_lattice_layers - 1):\n",
    "            lattice_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            lattice_layers.append(nn.ReLU())\n",
    "            if use_batchnorm:\n",
    "                lattice_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "        lattice_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        self.lattice_encoder = nn.Sequential(*lattice_layers)\n",
    "\n",
    "        # Extra feature encoder\n",
    "        self.extra_feat_encoder = nn.Sequential(\n",
    "            nn.Linear(extra_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final MLP\n",
    "        mlp_layers = []\n",
    "        in_dim = hidden_dim * 3  # x_pool + lattice_feat + extra_feat\n",
    "        for _ in range(num_mlp_layers - 1):\n",
    "            mlp_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            if use_batchnorm:\n",
    "                mlp_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            mlp_layers.append(nn.Dropout(p=dropout))\n",
    "            in_dim = hidden_dim\n",
    "        mlp_layers.append(nn.Linear(in_dim, num_classes))\n",
    "        self.final_mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch, lattice = (\n",
    "            data.x, data.edge_index, data.edge_attr, data.batch, data.lattice\n",
    "        )\n",
    "\n",
    "        edge_feat = self.edge_encoder(edge_attr)\n",
    "\n",
    "        for i, conv in enumerate(self.gnn_layers):\n",
    "            x = conv(x, edge_index, edge_feat)\n",
    "            x = F.relu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.gnn_bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x_pool = global_mean_pool(x, batch)\n",
    "\n",
    "        lattice_flat = lattice.reshape(-1, 9)\n",
    "        lattice_feat = self.lattice_encoder(lattice_flat)\n",
    "\n",
    "        extra_feat = torch.cat([\n",
    "            data.modified_scherrer,\n",
    "            data.microstrain,\n",
    "            data.oms\n",
    "        ], dim=1)\n",
    "        extra_feat = self.extra_feat_encoder(extra_feat)\n",
    "\n",
    "        out = torch.cat([x_pool, lattice_feat, extra_feat], dim=1)\n",
    "        out = self.final_mlp(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Train and eval functions \n",
    "\n",
    "\n",
    "########################################## training and eval\n",
    "# TRAINING FUNCTION\n",
    "\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device, target_name):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = torch.argmax(data[target_name], dim=1).long()\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device, target_name):\n",
    "    model.eval()\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    correct_top5 = 0\n",
    "    correct_top10 = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            \n",
    "            labels = torch.argmax(data[target_name], dim=1).long()\n",
    "            #labels = data[target_name]().long()\n",
    "            total += labels.size(0)\n",
    "            _, pred = out.topk(10, 1, True, True)\n",
    "\n",
    "            correct_top1 += (pred[:, :1] == labels.view(-1, 1)).sum().item()\n",
    "            correct_top3 += (pred[:, :3] == labels.view(-1, 1)).sum().item()\n",
    "            correct_top5 += (pred[:, :5] == labels.view(-1, 1)).sum().item()\n",
    "            correct_top10 += (pred[:, :10] == labels.view(-1, 1)).sum().item()\n",
    "\n",
    "    return {\n",
    "        \"top1_acc\": correct_top1 / total,\n",
    "        \"top3_acc\": correct_top3 / total,\n",
    "        \"top5_acc\": correct_top5 / total,\n",
    "        \"top10_acc\": correct_top10 / total,\n",
    "        \"macro_f1\": 0.0  # Calcolo F1 macro opzionale\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b9e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_in_dim = data_in[0].x.shape[1]\n",
    "edge_in_dim = data_in[0].edge_attr.shape[1]\n",
    "lattice_in_dim = 9\n",
    "extra_feat_dim = 3\n",
    "hidden_dim = 32\n",
    "dropout = 0.25\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "number_of_runs = [0,1,2,3,4]  # due seed come richiesto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a12f7b",
   "metadata": {},
   "source": [
    "### 3) Train the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3193e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms\"\n",
    "    print(f\"\\n===== Training config: {config_name} =====\")\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = MetalSaltGNN(\n",
    "        node_in_dim, edge_in_dim, lattice_in_dim, extra_feat_dim,\n",
    "        hidden_dim, num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=3,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    checkpoint_name = f\"tmp/Metal_salts_{config_name}.pt\"\n",
    "    patience = 50\n",
    "    eval_every = 5\n",
    "    best_metric = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        loss = train(model, train_loader, criterion, optimizer, device, \"metal_salts\")\n",
    "        if epoch % eval_every == 0:\n",
    "            res = evaluate(model, val_loader, device, \"metal_salts\")\n",
    "            macro_top_k = res[\"top5_acc\"]\n",
    "\n",
    "            if macro_top_k > best_metric:\n",
    "                best_metric = macro_top_k\n",
    "                epochs_no_improve = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_metric': best_metric\n",
    "                }, checkpoint_name)\n",
    "            else:\n",
    "                epochs_no_improve += eval_every\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement for {patience} evals).\")\n",
    "                break\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_name, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "\n",
    "    results.append({\n",
    "        'config': config_name,\n",
    "        'top1_acc': res_test['top1_acc'],\n",
    "        'top10_acc': res_test['top10_acc'],\n",
    "        'top5_acc': res_test['top5_acc'],\n",
    "        'top3_acc': res_test['top3_acc'],\n",
    "        'macro_f1': res_test['macro_f1']\n",
    "    })\n",
    "\n",
    "    print(f\"{config_name} TEST: top10_acc={res_test['top10_acc']:.4f}, top5_acc={res_test['top5_acc']:.4f}, top3_acc={res_test['top3_acc']:.4f}, macro_f1={res_test['macro_f1']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db04f4",
   "metadata": {},
   "source": [
    "### 4) Load and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22757017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluating config: HID32_DO0.25_SEED0_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms =====\n",
      "HID32_DO0.25_SEED0_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms TEST: top10_acc=0.6600, top5_acc=0.5409, top3_acc=0.4392, macro_f1=0.0000\n",
      "\n",
      "===== Evaluating config: HID32_DO0.25_SEED1_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms =====\n",
      "HID32_DO0.25_SEED1_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms TEST: top10_acc=0.6700, top5_acc=0.5533, top3_acc=0.4864, macro_f1=0.0000\n",
      "\n",
      "===== Evaluating config: HID32_DO0.25_SEED2_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms =====\n",
      "HID32_DO0.25_SEED2_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms TEST: top10_acc=0.6650, top5_acc=0.5484, top3_acc=0.4715, macro_f1=0.0000\n",
      "\n",
      "===== Evaluating config: HID32_DO0.25_SEED3_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms =====\n",
      "HID32_DO0.25_SEED3_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms TEST: top10_acc=0.6600, top5_acc=0.5558, top3_acc=0.4615, macro_f1=0.0000\n",
      "\n",
      "===== Evaluating config: HID32_DO0.25_SEED4_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms =====\n",
      "HID32_DO0.25_SEED4_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms TEST: top10_acc=0.6476, top5_acc=0.5509, top3_acc=0.4715, macro_f1=0.0000\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms\"\n",
    "    print(f\"\\n===== Evaluating config: {config_name} =====\")\n",
    "    \n",
    "    # Seed per riproducibilità\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Modello\n",
    "    \n",
    "    model = MetalSaltGNN(\n",
    "        node_in_dim, edge_in_dim, lattice_in_dim, extra_feat_dim,\n",
    "        hidden_dim, num_classes,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=3,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint_name = f\"tmp/Metal_salts_{config_name}.pt\"\n",
    "        # Carica best model e valuta su test\n",
    "    checkpoint = torch.load(checkpoint_name, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    res_test = evaluate(model, test_loader, device, \"metal_salts\")\n",
    "\n",
    "    # Logga i risultati\n",
    "    results.append({\n",
    "        'config': config_name,\n",
    "        'top1_acc': res_test['top1_acc'],\n",
    "        'top10_acc': res_test['top10_acc'],\n",
    "        'top5_acc': res_test['top5_acc'],\n",
    "        'top3_acc': res_test['top3_acc'],\n",
    "        'macro_f1': res_test['macro_f1']\n",
    "    })\n",
    "    print(f\"{config_name} TEST: top10_acc={res_test['top10_acc']:.4f}, top5_acc={res_test['top5_acc']:.4f}, top3_acc={res_test['top3_acc']:.4f}, macro_f1={res_test['macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89f6466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Config: HID32_DO0.25_X_edgeAttr_lattice_modScherrer_Microsstrain_Oms\n",
      "top1_acc \t= 0.31 ± 0.00\n",
      "top10_acc \t= 0.66 ± 0.01\n",
      "top5_acc \t= 0.55 ± 0.01\n",
      "top3_acc \t= 0.47 ± 0.02\n",
      "macro_f1 \t= 0.00 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# normalizza config togliendo il seed\n",
    "df['base_config'] = df['config'].str.replace(r'_SEED\\d+', '', regex=True)\n",
    "\n",
    "metrics = ['top1_acc','top10_acc','top5_acc','top3_acc','macro_f1']\n",
    "\n",
    "# calcolo mean e std per ogni base_config\n",
    "grouped = df.groupby('base_config')[metrics].agg(['mean','std'])\n",
    "\n",
    "# stampa formattata\n",
    "for cfg, row in grouped.iterrows():\n",
    "    print(f\"\\nConfig: {cfg}\")\n",
    "    for m in metrics:\n",
    "        mean = row[(m,'mean')]\n",
    "        std  = row[(m,'std')]\n",
    "        print(f\"{m} \\t= {mean:.2f} ± {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e9657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766db49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a8349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa363d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb3978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairmof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
