{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda9bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/miniconda3/envs/fairmof/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_pyg_obj\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import fix_target_shapes,remove_unused_onehot_columns,set_seed,filter_metals\n",
    "from mofstructure import mofdeconstructor\n",
    "\n",
    "from fairmofsyncondition.read_write.coords_library import pytorch_geometric_to_ase\n",
    "\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "\n",
    "convert_struct = {'cubic':0, 'hexagonal':1, 'monoclinic':2, 'orthorhombic':3, 'tetragonal':4,'triclinic':5, 'trigonal':6}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212130",
   "metadata": {},
   "source": [
    "# GNN Energy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afb0dd",
   "metadata": {},
   "source": [
    "### 1) Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d29a2",
   "metadata": {},
   "source": [
    "\n",
    "### to get oms\n",
    "### stru.get_oms()[\"has_oms\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b02499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mofstructure.structure import MOFstructure\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "convert_metals = {j:i for i,j in enumerate(mofdeconstructor.transition_metals()[1:])}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b9f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import LMDBDataset\n",
    "\n",
    "def load_pyg_obj_energy(path_to_mdb = \"mof_syncondition_data\"):    \n",
    "    data = []\n",
    "    for d in LMDBDataset(lmdb_path=path_to_mdb):\n",
    "        d.x = d.x.float()\n",
    "        data.append(d)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba5427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_pyg_obj_energy(path_to_mdb=\"../../data/stability_energy_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163b91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c151031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "# ==================== Utils & setup ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539df1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15bc7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Filtra classi rare (come nel tuo codice)\n",
    "Y = [d.energy for d in dataset]\n",
    "a,b = np.unique(Y, return_counts=True)\n",
    "conv_y = {i:j for i,j in zip(a,b)}\n",
    "\n",
    "\n",
    "\n",
    "# Split & loaders\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "y_train = np.array([d.energy for d in train_dataset])\n",
    "y_val = np.array([d.energy for d in val_dataset])\n",
    "y_test = np.array([d.energy for d in test_dataset])\n",
    "\n",
    "\n",
    "q_low, q_high = np.percentile(y_train, [1, 99])  # o [0.5, 99.5]\n",
    "def clip_like_train(arr):\n",
    "    return np.clip(arr, q_low, q_high)\n",
    "\n",
    "y_train_c = clip_like_train(y_train)\n",
    "y_val_c  = clip_like_train(y_val)\n",
    "y_test_c  = clip_like_train(y_test)\n",
    "\n",
    "# 3) Fit scaler SOLO sul train\n",
    "# scaler = StandardScaler()\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True)  # più robusto agli outlier\n",
    "scaler.fit(y_train_c.reshape(-1,1))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# salva\n",
    "with open(\"target_scaler_energy.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "    \n",
    "\n",
    "# 4) Trasforma y\n",
    "y_train_scaled = scaler.transform(y_train_c.reshape(-1,1)).ravel()\n",
    "y_val_scaled  = scaler.transform(y_val_c.reshape(-1,1)).ravel()\n",
    "y_test_scaled  = scaler.transform(y_test_c.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "def attach_scaled_energy(subset, scaled_values):\n",
    "    \"\"\"\n",
    "    subset: torch.utils.data.Subset (o lista di Data)\n",
    "    scaled_values: array 1D di float già scalati, stessa lunghezza del subset\n",
    "    \"\"\"\n",
    "    for data, y in zip(subset, scaled_values):\n",
    "        data.energy_scaled = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Aggiungi il nuovo campo a ciascun split\n",
    "attach_scaled_energy(train_dataset, y_train_scaled)\n",
    "attach_scaled_energy(val_dataset,   y_val_scaled)\n",
    "attach_scaled_energy(test_dataset,  y_test_scaled)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=128)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d939075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738c736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eaac62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# ==================== Helpers extras (immutati) ====================\n",
    "\n",
    "def _reshape_feat(tensor, d):\n",
    "    if hasattr(d, \"num_graphs\"):  # Batch\n",
    "        return tensor.view(d.num_graphs, -1)\n",
    "    else:  # singolo Data\n",
    "        return tensor.view(1, -1)\n",
    "\n",
    "EXTRA_GETTERS = {\n",
    "    \"atomic_one_hot\":      lambda d: _reshape_feat(d.atomic_one_hot, d),\n",
    "    \"space_group_number\":  lambda d: _reshape_feat(d.space_group_number, d),\n",
    "    \"crystal_system\":      lambda d: _reshape_feat(d.crystal_system, d),\n",
    "    \"oms\":                 lambda d: _reshape_feat(d.oms, d),\n",
    "    \"cordinates\":          lambda d: _reshape_feat(d.cordinates, d),  # (tenuto il nome come nel tuo dataset)\n",
    "}\n",
    "\n",
    "def compute_extras_dim(sample_data, selected_extras):\n",
    "    dim = 0\n",
    "    for name in selected_extras:\n",
    "        if name not in EXTRA_GETTERS:\n",
    "            raise ValueError(f\"Feature extra sconosciuta: {name}\")\n",
    "        dim += EXTRA_GETTERS[name](sample_data).shape[1]\n",
    "    return dim\n",
    "\n",
    "def build_extras_tensor(data, selected_extras):\n",
    "    if not selected_extras:\n",
    "        return None\n",
    "    parts = [EXTRA_GETTERS[name](data) for name in selected_extras]\n",
    "    return torch.cat(parts, dim=1)\n",
    "\n",
    "def extras_suffix(selected_extras):\n",
    "    if not selected_extras:\n",
    "        return \"no_extras\"\n",
    "    return \"_\".join(selected_extras)\n",
    "\n",
    "# ==================== Model (head -> regressione) ====================\n",
    "\n",
    "class MetalSaltGNN_Ablation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim,\n",
    "        edge_in_dim,\n",
    "        lattice_in_dim=9,\n",
    "        hidden_dim=128,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=None,\n",
    "        extras_dim=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout = dropout\n",
    "        self.selected_extras = selected_extras or []\n",
    "        self.extras_dim = extras_dim\n",
    "\n",
    "        # Edge encoder\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # GINE stack\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.gnn_bns = nn.ModuleList() if use_batchnorm else None\n",
    "        for i in range(num_gnn_layers):\n",
    "            in_dim = node_in_dim if i == 0 else hidden_dim\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            self.gnn_layers.append(GINEConv(mlp, edge_dim=hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                self.gnn_bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Lattice encoder\n",
    "        lattice_layers = []\n",
    "        in_dim = lattice_in_dim\n",
    "        for _ in range(max(1, num_lattice_layers - 1)):\n",
    "            lattice_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                lattice_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            in_dim = hidden_dim\n",
    "        lattice_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "        self.lattice_encoder = nn.Sequential(*lattice_layers)\n",
    "\n",
    "        # Final MLP head -> output 1 per regressione\n",
    "        final_in = hidden_dim * 2 + self.extras_dim\n",
    "        mlp_layers = []\n",
    "        in_dim = final_in\n",
    "        for _ in range(max(1, num_mlp_layers - 1)):\n",
    "            mlp_layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            if use_batchnorm:\n",
    "                mlp_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            mlp_layers.append(nn.Dropout(p=dropout))\n",
    "            in_dim = hidden_dim\n",
    "        mlp_layers.append(nn.Linear(in_dim, 1))  # <--- regressione\n",
    "        self.final_mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        e = self.edge_encoder(edge_attr)\n",
    "\n",
    "        for i, conv in enumerate(self.gnn_layers):\n",
    "            x = conv(x, edge_index, e)\n",
    "            x = F.relu(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.gnn_bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x_pool = global_mean_pool(x, batch)\n",
    "\n",
    "        lattice = data.lattice.view(-1, 9)\n",
    "        lattice_feat = self.lattice_encoder(lattice)\n",
    "\n",
    "        extras = build_extras_tensor(data, self.selected_extras)\n",
    "        if extras is not None:\n",
    "            final_in = torch.cat([x_pool, lattice_feat, extras], dim=1)\n",
    "        else:\n",
    "            final_in = torch.cat([x_pool, lattice_feat], dim=1)\n",
    "\n",
    "        out = self.final_mlp(final_in)        # shape [B, 1]\n",
    "        return out.squeeze(-1)                # shape [B]\n",
    "\n",
    "# ==================== Train / Eval per regressione ====================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, target_name=\"energy_scaled\"):\n",
    "    \"\"\"\n",
    "    Atteso: data[target_name] contiene il target SCALATO come float (shape [B] o [B,1])\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(data)  # [B]\n",
    "        target = data[target_name].view(-1).float()  # [B], scaled\n",
    "        loss = criterion(preds, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, target_name, scaler=None):\n",
    "    \"\"\"\n",
    "    Ritorna metriche sia nello spazio SCALATO (loss) sia, se fornito uno 'scaler',\n",
    "    nello spazio FISICO (MAE/RMSE/R2).\n",
    "    - scaler: lo stesso usato per scalare il target (fit sul TRAIN). Deve supportare inverse_transform.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    y_true_scaled, y_pred_scaled = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        preds = model(data)  # [B]\n",
    "        target = data[target_name].view(-1).float()\n",
    "        # se vuoi valutare la loss in scala del training (consigliato):\n",
    "        loss_batch = F.l1_loss(preds, target, reduction='mean')  # stessa loss del training se usi MAE\n",
    "        losses.append(loss_batch.item())\n",
    "\n",
    "        y_true_scaled.append(target.cpu())\n",
    "        y_pred_scaled.append(preds.cpu())\n",
    "\n",
    "    y_true_scaled = torch.cat(y_true_scaled).numpy()\n",
    "    y_pred_scaled = torch.cat(y_pred_scaled).numpy()\n",
    "\n",
    "    results = {\n",
    "        \"val_loss_scaled\": float(np.mean(losses)),  # ad es. MAE in scala z/robust\n",
    "    }\n",
    "\n",
    "    # Se hai lo scaler, riportiamo alle unità reali\n",
    "    if scaler is not None:\n",
    "        y_true = scaler.inverse_transform(y_true_scaled.reshape(-1,1)).ravel()\n",
    "        y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "        mae = np.mean(np.abs(y_pred - y_true))\n",
    "        rmse = float(np.sqrt(np.mean((y_pred - y_true)**2)))\n",
    "        r2 = float(r2_score(y_true, y_pred))\n",
    "\n",
    "        results.update({\n",
    "            \"MAE\": float(mae),\n",
    "            \"RMSE\": rmse,\n",
    "            \"R2\": r2,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ==================== Scelta loss/optimizer (esempio) ====================\n",
    "\n",
    "# Per outlier, spesso conviene Huber (smussata) o MAE:\n",
    "# criterion = nn.SmoothL1Loss(beta=1.0)  # Huber\n",
    "# oppure\n",
    "# criterion = nn.L1Loss()  # MAE, robusta agli outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6727f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dabf646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== Run ablation ====================\n",
    "\n",
    "node_in_dim = dataset[0].x.shape[1]\n",
    "edge_in_dim = dataset[0].edge_attr.shape[1]\n",
    "lattice_in_dim = 9\n",
    "\n",
    "# Scegli qui le extra da usare nell’ablation:\n",
    "# Esempio richiesto: [\"oms\", \"atomic_one_hot\"]\n",
    "\n",
    "EXTRA_GETTERS = {\n",
    "    \"atomic_one_hot\":      lambda d: _reshape_feat(d.atomic_one_hot, d),\n",
    "    \"space_group_number\":  lambda d: _reshape_feat(d.space_group_number, d),\n",
    "    \"crystal_system\":      lambda d: _reshape_feat(d.crystal_system, d),\n",
    "    \"oms\":                 lambda d: _reshape_feat(d.oms, d),\n",
    "    \"cordinates\":          lambda d: _reshape_feat(d.cordinates[0], d),\n",
    "}\n",
    "\n",
    "\n",
    "selected_extras = [\"atomic_one_hot\", \"cordinates\", \"oms\",\"space_group_number\"]\n",
    "# imoprtant: for saving\n",
    "selected_extras = np.sort(selected_extras).tolist()\n",
    "selected_extras\n",
    "\n",
    "# Calcolo dinamico della dimensione delle extra\n",
    "extras_dim = compute_extras_dim(dataset[0], selected_extras)\n",
    "\n",
    "# Classi\n",
    "number_of_runs = [1,2,3,4,5]\n",
    "hidden_dim = 64\n",
    "dropout = 0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "362efd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training config: HID64_DO0.35_SEED1__atomic_one_hot_cordinates_oms_space_group_number_energy =====\n",
      "train loss= -1028.3870 \t val loss=6.5364\n",
      "train loss= -3183.6568 \t val loss=16.8352\n",
      "train loss= -6157.3815 \t val loss=28.0092\n",
      "train loss= -10381.3831 \t val loss=54.3120\n",
      "train loss= -15868.2311 \t val loss=78.9346\n",
      "train loss= -21176.3532 \t val loss=95.5799\n",
      "Early stopping at epoch 6\n",
      "HID64_DO0.35_SEED1__atomic_one_hot_cordinates_oms_space_group_number_energy TEST: loss=6.4494\n",
      "\n",
      "===== Training config: HID64_DO0.35_SEED2__atomic_one_hot_cordinates_oms_space_group_number_energy =====\n",
      "train loss= -950.8655 \t val loss=7.4508\n",
      "train loss= -3031.6677 \t val loss=17.0024\n",
      "train loss= -6310.9382 \t val loss=30.9897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m eval_every = \u001b[32m1\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m1001\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % eval_every == \u001b[32m0\u001b[39m:\n\u001b[32m     34\u001b[39m         res = evaluate(model, val_loader, device, \u001b[33m\"\u001b[39m\u001b[33menergy_scaled\u001b[39m\u001b[33m\"\u001b[39m,scaler=scaler)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, target_name)\u001b[39m\n\u001b[32m    145\u001b[39m data = data.to(device)\n\u001b[32m    146\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B]\u001b[39;00m\n\u001b[32m    149\u001b[39m target = data[target_name].view(-\u001b[32m1\u001b[39m).float()  \u001b[38;5;66;03m# [B], scaled\u001b[39;00m\n\u001b[32m    150\u001b[39m loss = criterion(preds, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fairmof/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fairmof/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mMetalSaltGNN_Ablation.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    124\u001b[39m lattice = data.lattice.view(-\u001b[32m1\u001b[39m, \u001b[32m9\u001b[39m)\n\u001b[32m    125\u001b[39m lattice_feat = \u001b[38;5;28mself\u001b[39m.lattice_encoder(lattice)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m extras = \u001b[43mbuild_extras_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselected_extras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extras \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     final_in = torch.cat([x_pool, lattice_feat, extras], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mbuild_extras_tensor\u001b[39m\u001b[34m(data, selected_extras)\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     35\u001b[39m parts = [EXTRA_GETTERS[name](data) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m selected_extras]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "suffix = extras_suffix(selected_extras)\n",
    "\n",
    "for seed in number_of_runs:\n",
    "    config_name = f\"HID{hidden_dim}_DO{dropout}_SEED{seed}__{suffix}_energy\"\n",
    "    print(f\"\\n===== Training config: {config_name} =====\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = MetalSaltGNN_Ablation(\n",
    "        node_in_dim=node_in_dim,\n",
    "        edge_in_dim=edge_in_dim,\n",
    "        lattice_in_dim=lattice_in_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_gnn_layers=4,\n",
    "        num_lattice_layers=2,\n",
    "        num_mlp_layers=2,\n",
    "        dropout=dropout,\n",
    "        use_batchnorm=True,\n",
    "        selected_extras=selected_extras,\n",
    "        extras_dim=extras_dim).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    checkpoint_name = f\"trained_models/Metal_salts_{config_name}_tmp_test.pt\"\n",
    "\n",
    "    best_metric = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    patience = 5\n",
    "    eval_every = 1\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        if epoch % eval_every == 0:\n",
    "            res = evaluate(model, val_loader, device, \"energy_scaled\",scaler=scaler)\n",
    "            print(f\"train loss= {train_loss:.4f} \\t val loss={res['val_loss_scaled']:.4f}\")\n",
    "\n",
    "            # Early stopping su top5, come nel tuo codice\n",
    "            if res[\"val_loss_scaled\"] < best_metric:\n",
    "                best_metric = res[\"val_loss_scaled\"]\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), checkpoint_name)\n",
    "            else:\n",
    "                epochs_no_improve += eval_every\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # Valutazione test con il best checkpoint\n",
    "    model.load_state_dict(torch.load(checkpoint_name, map_location=device))\n",
    "    res_test = evaluate(model, test_loader, device, \"energy_scaled\")\n",
    "    results.append({**res_test, 'config': config_name})\n",
    "    print(f\"{config_name} TEST: loss={res_test['val_loss_scaled']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c978c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss_scaled': 7.151827010241422}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db399dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f18bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carica\n",
    "with open(\"target_scaler_energy.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "def inverse_to_physical(y_pred_scaled):\n",
    "    return scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "# Esempio PyTorch: loss su target scalato\n",
    "criterion = torch.nn.L1Loss()  # MAE spesso più robusta di MSE per outlier\n",
    "\n",
    "# y_batch_scaled: tensore target già scalato\n",
    "# pred_scaled: output della rete\n",
    "# loss = criterion(pred_scaled, y_batch_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4c661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ebd493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ac073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b55568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336d635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairmof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
